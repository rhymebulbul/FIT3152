---
output:
  pdf_document: default
  html_document: default
---
# FIT3152 - Data analytics

## Assignment 2

**Name:** Rhyme Bulbul

**Student ID:** 31865224

**AI Statement:** Generative AI was not used in this assignment.

<!-- - The main body of this report is almost 15 pages. This length is due to some long code blocks and the sizes of the decision tree diagram and ROC curve plots that have to be big to make them readable. All other pages are the Appendix, which include repeated code and outputs. -->
<!-- - Due to length, from Question 9 onwards, similar code from Questions 4 and 5 used to make predictions, create confusion matrices, report accuracies, construct ROC curves and compute AUC values will not be shown in the main report but in the Appendix. ROC curve plots will also not be shown in the main report except that of Question 12, which shows the curves for all classifiers. -->

<!-- My unique dataset of 2000 rows is first created with the following code from the specifications. Libraries needed to perform the tasks are imported as well. -->

We start by utilizing the R skeleton code to import out phishing dataset and set a seed using my student number so the unique data is replicable. Next, we take a random sample of 2000 rows of data, and import libraries further code will rely on.

```{r message = FALSE}
rm(list = ls())
Phish <- read.csv("PhishingData.csv")
set.seed(31865224)
L <- as.data.frame(c(1:50))
L <- L[sample(nrow(L), 10, replace = FALSE),]
Phish <- Phish[(Phish$A01 %in% L),]
PD <- Phish[sample(nrow(Phish), 2000, replace = FALSE),]

#install.packages("dplyr")
library(dplyr)
#install.packages("caret")
library(caret)
#install.packages("tree")
library(tree)
#install.packages("e1071")
library(e1071)
#install.packages("adabag")
library(adabag)
#install.packages("randomForest")
library(randomForest)
#install.packages("ROCR")
library(ROCR)
#install.packages("lightgbm")
library(lightgbm)
#install.packages("kernlab")
library(kernlab)
```

## Question 1

<!-- The proportion of days when it is more humid than the previous day, and the opposite, can be computed by subsetting the data based on the value of `Class`, which indicates whether the day in the next entry is more humid than that of the current one. -->

There are 729 phishing sites and 1271 legitimate sites in this sample. This brings the proportion of phishing sites to legitimate sites is 0.57, which indicates that a bit over half of the sites included in this sample of the data set are phishing sites. The data is relatively clean with no missing or empty values for this attribute.

```{r}
phishing <- nrow(subset(PD, subset = Class == 1))
legitimate <- nrow(subset(PD, subset = Class == 0))
phishing
legitimate
phishing / legitimate
nrow(PD[is.na(PD$Class), ])
```

<!-- Based on the results, the proportion is about 1.03, which means their respective frequencies are close to equal. The `Class` field is also empty for 109 entries, so the true proportion may differ slightly as roughly 5% of the data has no target variable value. -->

<!-- `summary()` is then run to get descriptions of the predictors. Due to its length, the output is shown in the Appendix. -->
Next, we will run a summary of the data set to depict predictor descriptions such mean and standard deviation, which can be found in the appendix. Due to the nature of this data set, we are only familiar with the representation of the `Class` column, as all other columns are numbered from `A01` to `A25`.

It is worth noting a large number of attributes including `AXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` have very low means, as well as quartiles of 0.00, which indicates they have a very low numerical value as well.
In contrast, `A12` has comparatively high mean and quartiles, indicating a higher numerical value in the dataset. In addition, all attributes except `A01` have a small number of missing values.

```{r results = "hide"}
summary(PD)
```

<!-- Among the numerical real-valued attributes, most of the means are between 2 and 22. Some exceptions are `WindGustSpeed`, with a mean of 43.62, and more notably, `Pressure9am` and `Pressure3pm`, with means more than 1000 (which is normal for the scale at which atmospheric pressure is measured). Some interesting mean values are `Rainfall`'s mean of 2.941 despite having a max value of 156.4, and `WindGustSpeed`'s mean despite having a max value of 130. In both cases, this indicates a presence of a few extreme values. -->

<!-- Additionally, one noteworthy observation is that there are NA values for every predictor except `Location`, with some predictors having significantly more than the rest (eg. `Sunshine` has 842 NA's compared to `Year`'s 22). -->

<!-- Then, this code is run to compute the standard deviations of the numerical attributes. -->

Next, analysing the numerical attributes standard deviation, we observe several columns including `AXXXXXXXXXXXXXXXXXXXXX` with an exquisitely low standard deviation, indicationg that those columns have been consistent in this sample of the dataset. On the flip side, it worth noting that several columns such as `AXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx` have particularly higher standard deviation, indicating disparity among the values in those columns.

```{r warning = FALSE}
apply(PD, 2, sd, na.rm = TRUE)
```

Considering the missing values and standard deviation, we should not be required to omit any attributes as the data set looks normal.
<!-- As expected, `WindGustSpeed` and `Rainfall` are the two real-valued attributes with the largest standard deviations due to their extreme values. The smallest standard deviation belongs to `Cloud3pm`, meaning that the fraction of sky obscured by cloud at 3pm has been consistent, for this dataset. -->

<!-- The only attributes that may, from first glance, be worth omitting are `Location` and `Year`, as a country's yearly climate is usually expected to be consistent. But due to Australia's size, diverse climate and geography, and climate change in recent years, both `Location` and `Year` might have a significant influence on `Class` and should be kept. All other predictors are closely related to the weather and are thus assumed to have predictive power on `Class` as well. -->


## Question 2

<!-- As all the predictors seem to have relatively equal importance based on their descriptions in the specifications (and it is generally a good idea to to build initial classifers using all attributes and skim them down from there), no columns are dropped. The only pre-processing required is to remove all rows with NA values to make the data suitable for model fitting. -->

<!-- The following code extracts only complete entries from `pd` into a new data frame called `pd_clean`, and `Class` is also converted to factors as its data type is numerical, which would result in a regression tree being built instead of a classification tree that classifies data points into 0 and 1. -->

Given none of the columns have any priority we can omit them based off, all columns are included. However, rows with missing values have to be dropped to make the data set suitable to have a model fitted to it. 

As such, we take a dataframe with the clean data, and convert the `Class` column into a factor, as it consists of a numerical data type and we are looking to build a classification tree rather than a regression tree. This now gives 1573 rows after dropping the ones with missing values.

```{r}
clean_pd <- PD[complete.cases(PD), ]
clean_pd$Class <- as.factor(clean_pd$Class)

nrow(clean_pd)
#table(clean_pd$Class)
```

<!-- The outputs of the last two lines shows that the processed dataset now contains 655 rows, and the number of unique locations has been reduced to 5. -->

## Question 3
<!-- The adapted code to divide my data with a 70% to 30% train-test split is as follows. -->
Adapting the given skeleton R code to divide the data into a 70% training and 30% test set, we get as follows

```{r}
train_row <- sample(1:nrow(clean_pd), 0.7 * nrow(clean_pd))
pd_train <- clean_pd[train_row, ]
pd_test <- clean_pd[-train_row, ]
```

## Question 4

<!-- A classification model of each technique is constructed with default settings. -->
We implement each of Decision Tree, Naive Baiyes, Bagging, Boosting and Random Forest below with R functions using their default settings.

```{r}
# Decision tree
pd_tree <- tree(Class ~ ., data = pd_train)

# Naive bayes
pd_bayes <- naiveBayes(Class ~ ., data = pd_train)

# Bagging
pd_bag <- bagging(Class ~ ., data = pd_train)

# Boosting
pd_boost <- boosting(Class ~ ., data = pd_train)

# Random forest
pd_forest <- randomForest(Class ~ ., data = pd_train)
```

## Question 5
`TODO!!!`
<!-- Each data point in the test data is classified into either `1` (more humid tomorrow) or `0` (less humid tomorrow) using `predict()` (`predict.bagging()` and `predict.boosting()` for bagging and boosting respectively). -->
We start classifying the data into either 1 for phishing sites, or 0 for legitimate sites using the predict functions for each respective models

```{r}
# decision tree
pd_tree_predict <- predict(pd_tree, pd_test, type = "class")

# naive bayes
pd_bayes_predict <- predict(pd_bayes, pd_test)

# bagging
pd_bag_predict <- predict.bagging(pd_bag, pd_test)

# boosting
pd_boost_predict <- predict.boosting(pd_boost, pd_test)

# random forest
pd_forest_predict <- predict(pd_forest, pd_test)
```

<!-- A confusion matrix is then created for each model, displayed alongside each model's accuracy, which is the total number of accurate predictions divided by total number of observations. A function is used to compute the accuracy for each confusion matrix, using the code `sum(diag(cm)) / sum(cm)`, where `cm` is a given confusion matrix. -->

Next, we develop a confusion matrix for each respective model, along with the accuracy of each. 

```{r}
get_accuracy <- function(confusion_matrix) {
    return(sum(diag(confusion_matrix)) / sum(confusion_matrix))
}

# decision tree
pd_tree_confusion_matrix <- table("Predicted Class" = pd_tree_predict,
                      "Actual Class" = pd_test$Class)
pd_tree_accuracy <- get_accuracy(pd_tree_confusion_matrix)
cat("Decision tree (accuracy:", pd_tree_accuracy, ")\n")
pd_tree_confusion_matrix

# naive bayes
pd_bayes_confusion_matrix <- table("Predicted Class" = pd_bayes_predict,
                       "Actual Class" = pd_test$Class)
pd_bayes_accuracy <- get_accuracy(pd_bayes_confusion_matrix)
cat("Naive bayes classifier (accuracy:", pd_bayes_accuracy, ")\n")
pd_bayes_confusion_matrix

# bagging
pd_bag_accuracy <- get_accuracy(pd_bag_predict$confusion)
cat("Bagging (accuracy:", pd_bag_accuracy, ")\n")
pd_bag_predict$confusion

# boosting
pd_boost_accuracy <- get_accuracy(pd_boost_predict$confusion)
cat("Boosting (accuracy:", pd_boost_accuracy, ")\n")
pd_boost_predict$confusion

# random forest
pd_forest_confusion_matrix <- table("Predicted Class" = pd_forest_predict,
                        "Actual Class" = pd_test$Class)
pd_forest_accuracy <- get_accuracy(pd_forest_confusion_matrix)
cat("Random forest (accuracy:", pd_forest_accuracy, ")\n")
pd_forest_confusion_matrix
```

<!-- Among the classifiers, only bagging and boosting have an accuracy above 0.6. In fact, they have identical accuracies (0.6345178). The other three classifiers have accuracies closer to that of random guessing, with the naive Bayes classifier having the worst accuracy at 0.5482234. -->

Overall, we find most models have accuracy of 0.7 or higher, while Naive Bayes classifier has the least of 0.39 which is on par with random guesses.


## Question 6

<!-- The confidence of predicting 'more humid tomorrow' is computed by adjusting the `type` parameter of the `predict()` function (`predict.bagging()` and `predict.boosting()` for bagging and boosting respectively) for each model. Their respective ROC curves is then computed using `prediction()` (which standardises the input data) and `performance()` (which evaluates a model's true positive rate and false positive rate). The ROC curves are then plotted on the same axis with different colours, and a diagonal line is added to represent the random classifier (for comparison purposes). -->

We calculate the confidence of predicting `phishing` for each case by utilizing the parameter `type` from the function predict already used in each model. Next, we are able to construct a ROC curve for each curve using the prediction and performance functions to plot them on the same axis using a different colour for each classifier.

```{r fig.height = 5, fig.width = 7}
# decision tree
pd_tree_predict_prob <- predict(pd_tree, pd_test, type = "vector")
pd_tree_pred <- prediction(pd_tree_predict_prob[, 2], pd_test$Class)
pd_tree_perf <- performance(pd_tree_pred, "tpr", "fpr")
plot(pd_tree_perf, col = "red")

# naive bayes
pd_bayes_predict_prob <- predict(pd_bayes, pd_test, type = "raw")
pd_bayes_pred <- prediction(pd_bayes_predict_prob[, 2], pd_test$Class)
pd_bayes_perf <- performance(pd_bayes_pred, "tpr", "fpr")
plot(pd_bayes_perf, col = "blue", add = TRUE)

# bagging
pd_bag_predict_prob <- predict.bagging(pd_bag, pd_test, type = "prob")
pd_bag_pred <- prediction(pd_bag_predict_prob$prob[, 2], pd_test$Class)
pd_bag_perf <- performance(pd_bag_pred, "tpr", "fpr")
plot(pd_bag_perf, col = "darkgreen", add = TRUE)

# boosting
pd_boost_predict_prob <- predict.boosting(pd_boost, pd_test, type = "prob")
pd_boost_pred <- prediction(pd_boost_predict_prob$prob[, 2], pd_test$Class)
pd_boost_perf <- performance(pd_boost_pred, "tpr", "fpr")
plot(pd_boost_perf, col = "violet", add = TRUE)

# random forest
pd_forest_predict_prob <- predict(pd_forest, pd_test, type = "prob")
pd_forest_pred <- prediction(pd_forest_predict_prob[, 2], pd_test$Class)
pd_forest_perf <- performance(pd_forest_pred, "tpr", "fpr")
plot(pd_forest_perf, col = "gold", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest"),
       col = c("red", "blue", "darkgreen", "violet", "gold"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict Class")
```

<!-- The AUC for each classifier is also calculated using `performance()`, but with the parameters adjusted to obtain the `auc` metric. -->

Next, we calculate the `AUC` for each classifier utilizing the performance function with `auc` as the parameter. `AUC` is computed to be above .6 for all models, as well as above .7 for Bagging and Random Forest.

```{r}
pd_tree_auc <- performance(pd_tree_pred, "auc")@y.values[[1]]
pd_bayes_auc <- performance(pd_bayes_pred, "auc")@y.values[[1]]
pd_bag_auc <- performance(pd_bag_pred, "auc")@y.values[[1]]
pd_boost_auc <- performance(pd_boost_pred, "auc")@y.values[[1]]
pd_forest_auc <- performance(pd_forest_pred, "auc")@y.values[[1]]

cat("Decision tree AUC", pd_tree_auc, "\n")
cat("Naive Bayes classifier AUC", pd_bayes_auc, "\n")
cat("Bagging AUC", pd_bag_auc, "\n")
cat("Boosting tree AUC", pd_boost_auc, "\n")
cat("Random forest AUC", pd_forest_auc, "\n")
```

<!-- If the classifiers are arranged in ascending order of AUC value, they would be in ascending order of accuracy as well, except that bagging and boosting have different AUC values. Boosting has an AUC value of 0.6351436, a little higher than that of bagging (0.6255202).  -->

## Question 7

<!-- A table to compare the results in Questions 5 and 6, namely the accuracies and AUC values of each classifier, is created in the form of a data frame, using the code as follows. -->

Creating a table comparing the AUC and accuracy of each model throughout questions 5 and 6, Random Forest wins the highest values, with both well above 0.7. However, it is not well ahead of Bagging, also both above 0.7, as well as Decision Tree and Boosting which are both above 0.6. Only Naives Bayes Classifier performs poorly as observed previously.

```{r}
pd_accuracy_auc <- data.frame(
  model = c("Decision tree", "Naive Bayes classifier", "Bagging", "Boosting",
            "Random forest"),
  accuracy = c(pd_tree_accuracy, pd_bayes_accuracy, pd_bag_accuracy, pd_boost_accuracy,
               pd_forest_accuracy),
  auc = c(pd_tree_auc, pd_bayes_auc, pd_bag_auc, pd_boost_auc, pd_forest_auc)
)

pd_accuracy_auc
```

<!-- From the output, there seems to exist a single "best" classifier based on the two metrics in focus, and that would be **boosting**. Both bagging and boosting have identical accuracies, but boosting has a slight edge due to its higher AUC value. However, it cannot be assumed now that boosting is definitely the single best classifier, due to the possibility of overfitting, which is a result of fitting too many attributes into a model. -->

## Question 8

We start by peeking through the summary of the decision tree for phishing data, with `A14 22 01 18 and 23` being the most important predictors of whether a website is a phishing one or is legitimate. Naives Bayes Classifiers, however assume each predictor to be independent and thus equal or no importance each.

```{r}
summary(pd_tree)
```

<!-- it is known that this classifier identifies the following attributes as the most important in predicting whether the next day would be more humid or not: `Sunshine`, `WindDir9am`, `WindGustDir`, `Rainfall`, `WindDir3pm`, `Cloud9am`, `Pressure9am`, `MinTemp` and `WindSpeed9am`. -->

<!-- As a naive Bayes classifier assumes independence between predictors, the predictors have no measure of importance. -->

<!-- For bagging, variable importance, in ascending order, can be displayed with the following code. -->

Sorting the variables in order of ascending importance for the Bagging model, we can see that `A01 22 18 23 14` are significantly higher in importance than the rest, with `A03 05 07 10 11 13 21 and 25` having no importance at all.

```{r}
sort(pd_bag$importance)
```

<!-- The output shows that `WindGustDir`, `WindDir3pm` and `WindDir9am` are significantly more important compared to the other attributes. The least important predictor among these three, `WindGustDir` (with a weightage of 18.8087757), alone is nearly three times more important compared to the next less important predictor, `Sunshine` (with a weightage of 6.7237428). -->

<!-- The process is repeated for boosting. -->

Sorting again in order of importance for the Boosting model, we get similar results, with `A22 18 23 01 08 24 and 12` being significantly higher than others, again with `A03 07 13 and 25` with no importance at all.

```{r}
sort(pd_boost$importance)
```

<!-- Similarly, the boosting classifier considers `WindGustDir`, `WindDir3pm` and `WindDir9am` as significantly more important than the other attributes. In a similar fashion to bagging, `WindGustDir` is the least important among these three and `Sunshine` is the next less important predictor. However, `Sunshine`'s weightage (5.3837980) is also nearly three times less than that of `WindGustDir` (15.8714548). -->

<!-- Finally, the variable importance for the random forest is inspected as well. -->
Finally, we also sort in order of importance for the Random Forest Model, this time however `A01 08 12 14 18 22 23 24` have significantly high values higher than 20, compared to the rest with much lower values.

```{r}
pd_forest$importance
```
To conclude, we find `A01 18 22 23` to have consistently high importance throughout all the models in predicting `Class`, while, `A03 07 13 25` have the least importance. Given the models in question have sufficiently high accuracy, these 4 least important variables could potentially be omitted from the data with little to no effect on performance. However, it worth noting that with trees based classifiers being unstable, minor differences in the input sampled can result in the tree generated varying significantly. As such, it not possible to predict the performance gain by omitting such variables, and is recommended to analyse the results after testing and training and make decisions based on that information.
<!-- Again, `WindGustDir`, `WindDir3pm` and `WindDir9am` are the most important, being the only predictors whose weightages are more than 20. -->

<!-- In conclusion, based on these classifiers, `WindGustDir`, `WindDir3pm` and `WindDir9am` are the most important variables in predicting MHT. The decision tree considers other attributes important as well, which are `Sunshine`, `Rainfall`, `Cloud9am`, `Pressure9am`, `MinTemp` and `WindSpeed9am`. However, the importance of these attributes may be exaggerated as the decision tree does not possess good performance metrics (both accuracy and AUC value are less than 0.6), and decision trees that have been fitted using all attributes tend to overfit. We also cannot assume other variables yet to be mentioned can be omitted from the data, as decision trees are unstable classifiers, meaning small variations in input data result in very different trees being generated. This applies to bagging, boosting and random forests as well, all of which are tree-based classifiers. Hence, it is not possible to determine which variables can be omitted from the data with very little effect on performance based on their importance weightages alone. Rather, the omission of different variables must be experimented with practically and the test results have to be compared. -->



## Question 9












































