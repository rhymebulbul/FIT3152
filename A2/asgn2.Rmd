# FIT3152 Data analytics

## Assignment 2

**Name:** Lim Yu-Shan

**Student ID:** 32685467

**Notes to marker:**

- The main body of this report is almost 15 pages. This length is due to some long code blocks and the sizes of the decision tree diagram and ROC curve plots that have to be big to make them readable. All other pages are the Appendix, which include repeated code and outputs.
- Due to length, from Question 9 onwards, similar code from Questions 4 and 5 used to make predictions, create confusion matrices, report accuracies, construct ROC curves and compute AUC values will not be shown in the main report but in the Appendix. ROC curve plots will also not be shown in the main report except that of Question 12, which shows the curves for all classifiers.

My unique dataset of 2000 rows is first created with the following code from the specifications. Libraries needed to perform the tasks are imported as well.

```{r message = FALSE}
rm(list = ls())
WAUS <- read.csv("HumidPredict2023D.csv", stringsAsFactors = TRUE)
L <- as.data.frame(c(1:49))
set.seed(32685467)
L <- L[sample(nrow(L), 10, replace = FALSE), ]
WAUS <- WAUS[(WAUS$Location %in% L), ]
WAUS <- WAUS[sample(nrow(WAUS), 2000, replace = FALSE), ]

library(dplyr)
library(caret)
library(tree)
library(e1071)
library(adabag)
library(randomForest)
library(ROCR)
library(lightgbm)
library(kernlab)
```

## Question 1

The proportion of days when it is more humid than the previous day, and the opposite, can be computed by subsetting the data based on the value of `MHT`, which indicates whether the day in the next entry is more humid than that of the current one.

```{r}
more <- nrow(subset(WAUS, subset = MHT == 1))
less <- nrow(subset(WAUS, subset = MHT == 0))
more
less
more / less
nrow(WAUS[is.na(WAUS$MHT), ])
```

Based on the results, the proportion is about 1.03, which means their respective frequencies are close to equal. The `MHT` field is also empty for 109 entries, so the true proportion may differ slightly as roughly 5% of the data has no target variable value.

`summary()` is then run to get descriptions of the predictors. Due to its length, the output is shown in the Appendix.

```{r results = "hide"}
summary(WAUS)
```

Among the numerical real-valued attributes, most of the means are between 2 and 22. Some exceptions are `WindGustSpeed`, with a mean of 43.62, and more notably, `Pressure9am` and `Pressure3pm`, with means more than 1000 (which is normal for the scale at which atmospheric pressure is measured). Some interesting mean values are `Rainfall`'s mean of 2.941 despite having a max value of 156.4, and `WindGustSpeed`'s mean despite having a max value of 130. In both cases, this indicates a presence of a few extreme values.

Additionally, one noteworthy observation is that there are NA values for every predictor except `Location`, with some predictors having significantly more than the rest (eg. `Sunshine` has 842 NA's compared to `Year`'s 22).

Then, this code is run to compute the standard deviations of the numerical attributes.

```{r warning = FALSE}
apply(WAUS, 2, sd, na.rm = TRUE)
```

As expected, `WindGustSpeed` and `Rainfall` are the two real-valued attributes with the largest standard deviations due to their extreme values. The smallest standard deviation belongs to `Cloud3pm`, meaning that the fraction of sky obscured by cloud at 3pm has been consistent, for this dataset.

The only attributes that may, from first glance, be worth omitting are `Location` and `Year`, as a country's yearly climate is usually expected to be consistent. But due to Australia's size, diverse climate and geography, and climate change in recent years, both `Location` and `Year` might have a significant influence on `MHT` and should be kept. All other predictors are closely related to the weather and are thus assumed to have predictive power on `MHT` as well.

## Question 2

As all the predictors seem to have relatively equal importance based on their descriptions in the specifications (and it is generally a good idea to to build initial classifers using all attributes and skim them down from there), no columns are dropped. The only pre-processing required is to remove all rows with NA values to make the data suitable for model fitting.

The following code extracts only complete entries from `WAUS` into a new data frame called `waus_clean`, and `MHT` is also converted to factors as its data type is numerical, which would result in a regression tree being built instead of a classification tree that classifies data points into 0 and 1.

```{r}
waus_clean <- WAUS[complete.cases(WAUS), ]
waus_clean$MHT <- as.factor(waus_clean$MHT)

nrow(waus_clean)
table(waus_clean$Location)
```

The outputs of the last two lines shows that the processed dataset now contains 655 rows, and the number of unique locations has been reduced to 5.

## Question 3

The adapted code to divide my data with a 70% to 30% train-test split is as follows.

```{r}
train_row <- sample(1:nrow(waus_clean), 0.7 * nrow(waus_clean))
waus_train <- waus_clean[train_row, ]
waus_test <- waus_clean[-train_row, ]
```

## Question 4

A classification model of each technique is constructed with default settings.

```{r}
# decision tree
waus_tree <- tree(MHT ~ ., data = waus_train)

# naive bayes
waus_bayes <- naiveBayes(MHT ~ ., data = waus_train)

# bagging
waus_bag <- bagging(MHT ~ ., data = waus_train)

# boosting
waus_boost <- boosting(MHT ~ ., data = waus_train)

# random forest
waus_forest <- randomForest(MHT ~ ., data = waus_train)
```

## Question 5

Each data point in the test data is classified into either `1` (more humid tomorrow) or `0` (less humid tomorrow) using `predict()` (`predict.bagging()` and `predict.boosting()` for bagging and boosting respectively).

```{r}
# decision tree
waus_tree_predict <- predict(waus_tree, waus_test, type = "class")

# naive bayes
waus_bayes_predict <- predict(waus_bayes, waus_test)

# bagging
waus_bag_predict <- predict.bagging(waus_bag, waus_test)

# boosting
waus_boost_predict <- predict.boosting(waus_boost, waus_test)

# random forest
waus_forest_predict <- predict(waus_forest, waus_test)
```

A confusion matrix is then created for each model, displayed alongside each model's accuracy, which is the total number of accurate predictions divided by total number of observations. A function is used to compute the accuracy for each confusion matrix, using the code `sum(diag(cm)) / sum(cm)`, where `cm` is a given confusion matrix.

```{r}
get_accuracy <- function(cm) {
    return(sum(diag(cm)) / sum(cm))
}

# decision tree
waus_tree_cm <- table("Predicted Class" = waus_tree_predict,
                      "Actual Class" = waus_test$MHT)
waus_tree_acc <- get_accuracy(waus_tree_cm)
cat("Decision tree (accuracy:", waus_tree_acc, ")\n")
waus_tree_cm

# naive bayes
waus_bayes_cm <- table("Predicted Class" = waus_bayes_predict,
                       "Actual Class" = waus_test$MHT)
waus_bayes_acc <- get_accuracy(waus_bayes_cm)
cat("Naive bayes classifier (accuracy:", waus_bayes_acc, ")\n")
waus_bayes_cm

# bagging
waus_bag_acc <- get_accuracy(waus_bag_predict$confusion)
cat("Bagging (accuracy:", waus_bag_acc, ")\n")
waus_bag_predict$confusion

# boosting
waus_boost_acc <- get_accuracy(waus_boost_predict$confusion)
cat("Boosting (accuracy:", waus_boost_acc, ")\n")
waus_boost_predict$confusion

# random forest
waus_forest_cm <- table("Predicted Class" = waus_forest_predict,
                        "Actual Class" = waus_test$MHT)
waus_forest_acc <- get_accuracy(waus_forest_cm)
cat("Random forest (accuracy:", waus_forest_acc, ")\n")
waus_forest_cm
```

Among the classifiers, only bagging and boosting have an accuracy above 0.6. In fact, they have identical accuracies (0.6345178). The other three classifiers have accuracies closer to that of random guessing, with the naive Bayes classifier having the worst accuracy at 0.5482234.

## Question 6

The confidence of predicting 'more humid tomorrow' is computed by adjusting the `type` parameter of the `predict()` function (`predict.bagging()` and `predict.boosting()` for bagging and boosting respectively) for each model. Their respective ROC curves is then computed using `prediction()` (which standardises the input data) and `performance()` (which evaluates a model's true positive rate and false positive rate). The ROC curves are then plotted on the same axis with different colours, and a diagonal line is added to represent the random classifier (for comparison purposes).

```{r fig.height = 5, fig.width = 7}
# decision tree
waus_tree_predict_prob <- predict(waus_tree, waus_test, type = "vector")
waus_tree_pred <- prediction(waus_tree_predict_prob[, 2], waus_test$MHT)
waus_tree_perf <- performance(waus_tree_pred, "tpr", "fpr")
plot(waus_tree_perf, col = "red")

# naive bayes
waus_bayes_predict_prob <- predict(waus_bayes, waus_test, type = "raw")
waus_bayes_pred <- prediction(waus_bayes_predict_prob[, 2], waus_test$MHT)
waus_bayes_perf <- performance(waus_bayes_pred, "tpr", "fpr")
plot(waus_bayes_perf, col = "blue", add = TRUE)

# bagging
waus_bag_predict_prob <- predict.bagging(waus_bag, waus_test, type = "prob")
waus_bag_pred <- prediction(waus_bag_predict_prob$prob[, 2], waus_test$MHT)
waus_bag_perf <- performance(waus_bag_pred, "tpr", "fpr")
plot(waus_bag_perf, col = "darkgreen", add = TRUE)

# boosting
waus_boost_predict_prob <- predict.boosting(waus_boost, waus_test, type = "prob")
waus_boost_pred <- prediction(waus_boost_predict_prob$prob[, 2], waus_test$MHT)
waus_boost_perf <- performance(waus_boost_pred, "tpr", "fpr")
plot(waus_boost_perf, col = "violet", add = TRUE)

# random forest
waus_forest_predict_prob <- predict(waus_forest, waus_test, type = "prob")
waus_forest_pred <- prediction(waus_forest_predict_prob[, 2], waus_test$MHT)
waus_forest_perf <- performance(waus_forest_pred, "tpr", "fpr")
plot(waus_forest_perf, col = "gold", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest"),
       col = c("red", "blue", "darkgreen", "violet", "gold"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")
```

The AUC for each classifier is also calculated using `performance()`, but with the parameters adjusted to obtain the `auc` metric.

```{r}
waus_tree_auc <- performance(waus_tree_pred, "auc")@y.values[[1]]
waus_bayes_auc <- performance(waus_bayes_pred, "auc")@y.values[[1]]
waus_bag_auc <- performance(waus_bag_pred, "auc")@y.values[[1]]
waus_boost_auc <- performance(waus_boost_pred, "auc")@y.values[[1]]
waus_forest_auc <- performance(waus_forest_pred, "auc")@y.values[[1]]

cat("Decision tree AUC", waus_tree_auc, "\n")
cat("Naive Bayes classifier AUC", waus_bayes_auc, "\n")
cat("Bagging AUC", waus_bag_auc, "\n")
cat("Boosting tree AUC", waus_boost_auc, "\n")
cat("Random forest AUC", waus_forest_auc, "\n")
```

If the classifiers are arranged in ascending order of AUC value, they would be in ascending order of accuracy as well, except that bagging and boosting have different AUC values. Boosting has an AUC value of 0.6351436, a little higher than that of bagging (0.6255202). 

## Question 7

A table to compare the results in Questions 5 and 6, namely the accuracies and AUC values of each classifier, is created in the form of a data frame, using the code as follows.

```{r}
waus_acc_auc <- data.frame(
  model = c("Decision tree", "Naive Bayes classifier", "Bagging", "Boosting",
            "Random forest"),
  accuracy = c(waus_tree_acc, waus_bayes_acc, waus_bag_acc, waus_boost_acc,
               waus_forest_acc),
  auc = c(waus_tree_auc, waus_bayes_auc, waus_bag_auc, waus_boost_auc, waus_forest_auc)
)

waus_acc_auc
```

From the output, there seems to exist a single "best" classifier based on the two metrics in focus, and that would be **boosting**. Both bagging and boosting have identical accuracies, but boosting has a slight edge due to its higher AUC value. However, it cannot be assumed now that boosting is definitely the single best classifier, due to the possibility of overfitting, which is a result of fitting too many attributes into a model.

## Question 8

By inspecting the decision tree as follows,

```{r}
summary(waus_tree)
```

it is known that this classifier identifies the following attributes as the most important in predicting whether the next day would be more humid or not: `Sunshine`, `WindDir9am`, `WindGustDir`, `Rainfall`, `WindDir3pm`, `Cloud9am`, `Pressure9am`, `MinTemp` and `WindSpeed9am`.

As a naive Bayes classifier assumes independence between predictors, the predictors have no measure of importance.

For bagging, variable importance, in ascending order, can be displayed with the following code.

```{r}
sort(waus_bag$importance)
```

The output shows that `WindGustDir`, `WindDir3pm` and `WindDir9am` are significantly more important compared to the other attributes. The least important predictor among these three, `WindGustDir` (with a weightage of 18.8087757), alone is nearly three times more important compared to the next less important predictor, `Sunshine` (with a weightage of 6.7237428).

The process is repeated for boosting.

```{r}
sort(waus_boost$importance)
```

Similarly, the boosting classifier considers `WindGustDir`, `WindDir3pm` and `WindDir9am` as significantly more important than the other attributes. In a similar fashion to bagging, `WindGustDir` is the least important among these three and `Sunshine` is the next less important predictor. However, `Sunshine`'s weightage (5.3837980) is also nearly three times less than that of `WindGustDir` (15.8714548).

Finally, the variable importance for the random forest is inspected as well.

```{r}
waus_forest$importance
```

Again, `WindGustDir`, `WindDir3pm` and `WindDir9am` are the most important, being the only predictors whose weightages are more than 20.

In conclusion, based on these classifiers, `WindGustDir`, `WindDir3pm` and `WindDir9am` are the most important variables in predicting MHT. The decision tree considers other attributes important as well, which are `Sunshine`, `Rainfall`, `Cloud9am`, `Pressure9am`, `MinTemp` and `WindSpeed9am`. However, the importance of these attributes may be exaggerated as the decision tree does not possess good performance metrics (both accuracy and AUC value are less than 0.6), and decision trees that have been fitted using all attributes tend to overfit. We also cannot assume other variables yet to be mentioned can be omitted from the data, as decision trees are unstable classifiers, meaning small variations in input data result in very different trees being generated. This applies to bagging, boosting and random forests as well, all of which are tree-based classifiers. Hence, it is not possible to determine which variables can be omitted from the data with very little effect on performance based on their importance weightages alone. Rather, the omission of different variables must be experimented with practically and the test results have to be compared.

## Question 9

To create a simple classifier for a person to make classifications by hand, I started from the decision tree created in Question 4. The approach here is to prune the tree to an optimal size, yielding a simpler decision tree that can be used to make classifications easily.

To determine what size to prune the tree to, cross-validation is carried out on the original decision tree using `cv.tree()`. All attributes from the original tree were preserved, as the tree was trained on all attributes and, despite being possibly overfitted, is the most "complete" (contains the most information) tree to prune. In other words, **post-pruning** will be performed on the original tree to obtain the simple tree.

```{r}
waus_tree_cv <- cv.tree(waus_tree)
waus_tree_cv
```

Based on the output, the tree size that yields the lowest deviance is 1. However, a tree with a single terminal node does not make sense. The next "best" tree size is 2, which is quite clearly an underfitted tree that is far too simple. Despite having the lowest deviances, trees of these sizes are expected to perform poorly on unseen data. The low deviances are most probably due to cross-validation finding such simple trees working well and overfitting on the training data. The optimal tree size, thus, cannot be selected based on the lowest deviance alone.

The tree size with the next lowest deviance is 7. This is chosen, as it is a reasonable tree size that is not too large or small, and still yields a relatively low deviance. `prune.tree()` is used to prune the tree.

```{r fig.height = 5, fig.width = 7}
waus_tree_pruned <- prune.tree(waus_tree, best = 7)
plot(waus_tree_pruned)
text(waus_tree_pruned, pretty = 0)
```

The resulting decision tree is simple and easy to interpret. Consisting of only 7 terminal nodes, it splits on only 4 attributes, compared to the original decision tree which has 22 terminal nodes and splits on 9 attributes.

Similar code from Question 5 is used to evaluate the performance of this pruned decision tree on the test data, create a confusion matrix and report its accuracy.

```{r include = FALSE}
waus_tree_pruned_predict <- predict(waus_tree_pruned, waus_test, type = "class")
waus_tree_pruned_cm <- table("Predicted Class" = waus_tree_pruned_predict,
                             "Actual Class" = waus_test$MHT)
waus_tree_pruned_acc <- get_accuracy(waus_tree_pruned_cm)
```

```{r}
waus_tree_pruned_acc
waus_tree_pruned_cm
```

The accuracy of this pruned decision tree is 0.5634518, which is a slight improvement of that of the original tree (0.5532995).

Similar code from Question 6 is then used to update the plot of ROC curves and compute the AUC value (see Appendix).

```{r include = FALSE, results = "hide"}
waus_tree_pruned_predict_prob <- predict(waus_tree_pruned, waus_test, type = "vector")
waus_tree_pruned_pred <- prediction(waus_tree_pruned_predict_prob[, 2], waus_test$MHT)
waus_tree_pruned_perf <- performance(waus_tree_pruned_pred, "tpr", "fpr")

plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")

waus_tree_pruned_auc <- performance(waus_tree_pruned_pred, "auc")@y.values[[1]]
```

```{r}
waus_acc_auc <- rbind(waus_acc_auc, data.frame(model = "Pruned decision tree",
                                               accuracy = waus_tree_pruned_acc,
                                               auc = waus_tree_pruned_auc))
waus_acc_auc                                              
```

Again, a slight improvement of the AUC value from the original decision tree's 0.5761548 to the pruned tree's 0.5838535 is observed. Most of this improvement is seen at low to medium-high thresholds. When the threshold is high, the pruned tree noticeably performs worse than random guessing.

## Question 10

In my attempts to create the best tree-based classifier, I have tried adjusting parameters, omitting predictors, cross-validation, and scaling features on the classifiers created in Question 4. Unfortunately, no resulting classifier was able to beat the performance of the boosting and bagging classifiers built in Question 4 with default parameters. Instead, the strongest tree-based classifier I could create is a gradient-boosted trees model (note: I have sought confirmation from Dr Betts that a new type of tree-based classifier can be used for this question).

Gradient boosting is an ensemble method that iteratively trains decision trees to to minimise the errors made by the ensemble of previously-trained trees. The functions to construct a gradient-boosted trees model is provided by the [`lightgbm`](https://cran.r-project.org/web/packages/lightgbm/index.html) package.

Gradient boosting only accepts numerical input features and targets. Hence, the factor variables in the training and testing data are first encoded into numerical values.

```{r}
waus_train_lgbm <- waus_train
waus_test_lgbm <- waus_test

for (col in colnames(waus_train_lgbm)[1:21]) {
  if (is.factor(waus_train_lgbm[, col])) {
    waus_train_lgbm[, col] <- as.numeric(waus_train_lgbm[, col])
    waus_test_lgbm[, col] <- as.numeric(waus_test_lgbm[, col])
  }
}

waus_train_lgbm$MHT <- as.numeric(waus_train_lgbm$MHT) - 1
waus_test_lgbm$MHT <- as.numeric(waus_test_lgbm$MHT) - 1
```

The function that fits the model is `lgb.train()`, which requires the input data to be of a special class called `lgb.Dataset`. The following code adapts the training data to this class.

```{r}
waus_train_lgbmd <- lgb.Dataset(data = as.matrix(waus_train_lgbm[1:21]),
                                label = as.matrix(waus_train_lgbm$MHT))
```

The following parameters were used to get the best-performing gradient-boosted trees model, which is first assigned to a variable to be passed into `lgb.train()`. The parameter values were selected based on repetitive attempts with different values to get the best-performing model.

- `objective = "binary"`: sets `lgb.train()` to train a binary classfication model
- `learning_rate = 0.05`: sets the shrinkage rate (controls how much each tree in the ensemble contributes to the final prediction) to 0.05
- `feature_fraction = 0.7`: sets `lgb.train()` to randomly select 70% of features on each tree node
- `bagging_fraction = 0.8`: sets `lgb.train()` to select 80% of data without resampling for bagging
- `bagging_freq = 10`: sets `lgb.train()` to perform bagging at every 10 iterations

```{r}
params <- list(objective = "binary", learning_rate = 0.05, feature_fraction = 0.7,
               bagging_fraction = 0.8, bagging_freq = 10)
```

`lgb.train()` is then run with these parameters and 100 rounds.

```{r results = "hide"}
waus_lgbm <- lgb.train(params = params, data = waus_train_lgbmd, nrounds = 100)
```

The confusion matrix is created and the accuracy is reported.

```{r include = FALSE, message = FALSE}
waus_lgbm_predict <- predict(waus_lgbm, as.matrix(waus_test_lgbm[1:21]),
                             type = "response")
waus_lgbm_cm <- table("Predicted Class" = ifelse(waus_lgbm_predict > 0.5, 1, 0),
                     "Actual Class" = waus_test_lgbm$MHT)
waus_lgbm_acc <- get_accuracy(waus_lgbm_cm)
```

```{r}
waus_lgbm_acc
waus_lgbm_cm
```

The accuracy of the gradient-boosted trees model is 0.6548223, which outperforms every tree-based classifier we have thus far. As in previous questions, the ROC curve for this model is added to the plot and the AUC value is computed (see Appendix).

```{r include = FALSE, results = "hide"}
waus_lgbm_pred <- prediction(waus_lgbm_predict, waus_test_lgbm$MHT)
waus_lgbm_perf <- performance(waus_lgbm_pred, "tpr", "fpr")

plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)
plot(waus_lgbm_perf, col = "green", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree", "Gradient-boosted trees"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue", "green"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")

waus_lgbm_auc <- performance(waus_lgbm_pred, "auc")@y.values[[1]]
waus_acc_auc <- rbind(waus_acc_auc,
                      data.frame(model = "Gradient-boosted trees",
                                 accuracy = waus_lgbm_acc,
                                 auc = waus_lgbm_auc))
```

```{r}
waus_acc_auc
```

The AUC value is 0.6369122, which is also higher than that of every tree-based classifier thus far. Based on the ROC curve, this model performs worse than bagging and boosting at medium-high thresholds, but better at all other thresholds. These metrics show that this classifier is indeed better than the others for predictions of MHT.

The reason why gradient boosting was chosen when none of the other tree-based classifiers were able to improve was because gradient boosting is a variant of boosting, which is the most powerful classifier in Question 4. By implementing a possibly better variant of it that tends less to overfit and is good at capturing relationships between predictors, an improvement in performance is expected. Gradient boosting has also been known for strong performance in binary classification tasks in general. All original attributes of `WAUS` were retained as including all of them produced the best models for bagging and boosting. Of course, the factor predictors and target variable have to be first encoded into numerical values. 

## Question 11

To implement an artificial neural network (ANN) classifier, the `neuralnet` package is used. 

```{r message = FALSE}
library(neuralnet)
```

As ANNs only accept numeric inputs (and only predict numeric outputs), some pre-processing is required. First, all factor attributes (which were originally strings) are dropped from the data frame. This gives a remainder of 17 predictors, which is still enough to construct a reasonably good model (encoding factors to numerics was attempted, but the resulting ANN had significantly worse performance). The `MHT` column, which was converted to factors to construct the models in Question 4, is converted back to numerics as well. All the predictors are also normalised using `scale()`. Skipping this step may result in the ANN algorithm not converging before the maximum number of iterations is reached. 

```{r}
waus_train_nn <- waus_train[, c(1, 2, 3, 4, 5, 6, 7, 9, 12, 13, 14, 15, 16, 17, 18, 19,
                                21, 22)]
waus_test_nn <- waus_test[, c(1, 2, 3, 4, 5, 6, 7, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21,
                              22)]

waus_train_nn$MHT <- as.numeric(waus_train_nn$MHT) - 1
waus_test_nn$MHT <- as.numeric(waus_test_nn$MHT) - 1

waus_train_nn[1:17] <- scale(waus_train_nn[1:17])
waus_test_nn[1:17] <- scale(waus_test_nn[1:17])
```

Now, the pre-processed data can be used to construct the ANN using `neuralnet()`. As in Question 5, the test data is used for predictions. A confusion matrix is drawn and the accuracy is reported.

```{r include = FALSE}
waus_nn <- neuralnet(MHT == 1 ~ ., waus_train_nn, hidden = 3, linear.output = FALSE)
waus_nn_predict <- predict(waus_nn, waus_test_nn[, 1:17])
waus_nn_cm <- table("Predicted Class" = ifelse(waus_nn_predict > 0.5, 1, 0),
                    "Actual Class" = waus_test_nn$MHT)
waus_nn_acc <- get_accuracy(waus_nn_cm)
```

```{r}
waus_nn_acc
waus_nn_cm
```

The accuracy of the ANN is 0.6091371, which joins bagging, boosting and gradient-boosted trees as a classifier with an accuracy over 0.6.

To get the ROC curve and AUC value of the ANN, the `neuralnet` package has to be first detached as it overrides the `ROCR` package's `prediction()` method.

```{r message = FALSE}
detach(package:neuralnet, unload = TRUE)
```

As the predictions of the ANN are in the form of continuous values between 0 and 1, they effectively represent probabilities of a "1" classification as well. Thus, `waus_nn_predict` is passed directly into `prediction()`. The plot of ROC curves is updated and the AUC value is computed (see Appendix).

```{r include = FALSE, results = "hide"}
waus_nn_pred <- prediction(waus_nn_predict, waus_test_nn$MHT)
waus_nn_perf <- performance(waus_nn_pred, "tpr", "fpr")

plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)
plot(waus_lgbm_perf, col = "green", add = TRUE)
plot(waus_nn_perf, col = "chocolate", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree", "Gradient-boosted trees", "ANN"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue", "green",
               "chocolate"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")

waus_nn_auc <- performance(waus_nn_pred, "auc")@y.values[[1]]
waus_acc_auc <- rbind(waus_acc_auc,
                      data.frame(model = "Artificial neural network",
                                 accuracy = waus_nn_acc, auc = waus_nn_auc))
```

```{r}
waus_acc_auc
```

The AUC value is 0.6852892, which is the best among all the classifiers we have thus far. The ROC curve shows that the ANN is the strongest classifier at high and medium-high thresholds, before gradually being outperformed by gradient-boosted trees further up the curve. The reason why the AUC value is the highest among all classifiers but the accuracy is not relatively high, may be due to the ANN predicting positives (`MHT = 1`) well but not negatives, as shown by the confusion matrix. Having a high true positive rate contributes to the ANN's higher AUC, but due to mediocre performance on the negative class, the overall accuracy of the model is relatively medium.

## Question 12

The new classifier is a support vector machine (SVM) implemented using the R package [`kernlab`](https://cran.r-project.org/web/packages/kernlab/index.html).

An SVM is a supervised learning model that is applicable to both classification and regression, making it a suitable model for this assignment's data. In binary classification, the SVM attempts to find a hyperplane, or decision boundary, that separates the training data, putting each instance into one of two classes. During training, the hyperplane is formed based on the selection of support vectors, which are the instances from each class that are closest to each other in the feature space. The aim is to maximise the distance between these points and the hyperplane, forming as distinct a separation as possible. SVMs can also implement a kernel function, which transforms the input into a higher-dimensional space. A hyperplane of a higher dimension can thus be found, which may be better at separating the instances.

The method provided by `kernlab` that fits the SVM model is `ksvm()`. The kernel used is the linear kernel, represented by the argument `vanilladot`. This was chosen due to the large number of predictors, which decreases the separability of the data points in a higher dimensional space. 

```{r results = "hide"}
waus_svm <- ksvm(MHT ~ ., data = waus_train, kernel = "vanilladot")
```

```{r include = FALSE}
waus_svm_predict <- predict(waus_svm, waus_test)
waus_svm_cm <- table("Predicted Class" = waus_svm_predict, "Actual Class" = waus_test$MHT)
waus_svm_acc <- get_accuracy(waus_svm_cm)
```

The confusion matrix is then constructed and the accuracy is reported.

```{r}
waus_svm_acc
waus_svm_cm
```

The accuracy of the model is 0.6395939, which is better than that of all classifiers except gradient-boosted trees.

To get the ROC curve and AUC value of the model, `ksvm()` is re-executed to include the `prob.model = TRUE` argument, which gives us the confidence values of whether it is more humid the next day. The same process for the earlier classifiers is then repeated (see Appendix). An updated plot of ROC curves is drawn.

```{r include = FALSE, results = "hide"}
waus_svm_prob <- ksvm(MHT ~ ., data = waus_train, kernel = "vanilladot",
                      prob.model = TRUE)
```

```{r echo = FALSE, fig.height = 5, fig.width = 7}
waus_svm_predict_prob <- predict(waus_svm_prob, waus_test, type = "prob")
waus_svm_pred <- prediction(waus_svm_predict_prob[, 2], waus_test$MHT)
waus_svm_perf <- performance(waus_svm_pred, "tpr", "fpr")

plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)
plot(waus_lgbm_perf, col = "green", add = TRUE)
plot(waus_nn_perf, col = "chocolate", add = TRUE)
plot(waus_svm_perf, col = "grey", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree", "Gradient-boosted trees", "ANN", "SVM"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue", "green", "chocolate",
               "grey"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")

waus_svm_auc <- performance(waus_svm_pred, "auc")@y.values[[1]]
waus_acc_auc <- rbind(waus_acc_auc, data.frame(model = "Support vector machine",
                                               accuracy = waus_svm_acc,
                                               auc = waus_svm_auc))
```

```{r}
waus_acc_auc
```

The AUC value is 0.6493966, which exceeds that of all classifiers except the artificial neural network. Based on the ROC curve, this classifier outperforms all others at low to medium-low thresholds. This makes the SVM a very good option for predicting `MHT` in situations where a larger number of false positives is tolerated.

# Appendix

Output of `summary(WAUS)` in Question 1.

```{r echo = FALSE}
summary(WAUS)
```

Diagram of initial decision tree plotted in Question 4.

```{r fig.height = 9, fig.width = 7}
plot(waus_tree)
text(waus_tree, pretty = 0)
```

Code to make predictions, create confusion matrix, report accuracy, construct ROC curve and compute AUC value for the simple classifier (pruned decision tree) in Question 9.

```{r eval = FALSE}
waus_tree_pruned_predict <- predict(waus_tree_pruned, waus_test, type = "class")
waus_tree_pruned_cm <- table("Predicted Class" = waus_tree_pruned_predict,
                             "Actual Class" = waus_test$MHT)
waus_tree_pruned_acc <- get_accuracy(waus_tree_pruned_cm)

waus_tree_pruned_predict_prob <- predict(waus_tree_pruned, waus_test, type = "vector")
waus_tree_pruned_pred <- prediction(waus_tree_pruned_predict_prob[, 2],  waus_test$MHT)
waus_tree_pruned_perf <- performance(waus_tree_pruned_pred, "tpr", "fpr")

plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")

waus_tree_pruned_auc <- performance(waus_tree_pruned_pred, "auc")@y.values[[1]]
```

ROC curves at Queestion 9.

```{r echo = FALSE, fig.height = 5, fig.width = 7}
plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree"),
       col = c("red", "blue", "darkgreen", "violet", "gold",
               "skyblue"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")
```

Code to make predictions, create confusion matrix, report accuracy, construct ROC curve, compute AUC value and update classifier comparison table for the best tree-based classifier (gradient-boosted trees) in Question 10.

```{r eval = FALSE}
waus_lgbm_predict <- predict(waus_lgbm, as.matrix(waus_test_lgbm[1:21]),
                             type = "response")
waus_lgbm_cm <- table("Predicted Class" = ifelse(waus_lgbm_predict > 0.5, 1, 0),
                     "Actual Class" = waus_test_lgbm$MHT)
waus_lgbm_acc <- get_accuracy(waus_lgbm_cm)
waus_lgbm_pred <- prediction(waus_lgbm_predict, waus_test_lgbm$MHT)
waus_lgbm_perf <- performance(waus_lgbm_pred, "tpr", "fpr")

plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)
plot(waus_lgbm_perf, col = "green", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree", "Gradient-boosted trees"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue", "green"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")

waus_lgbm_auc <- performance(waus_lgbm_pred, "auc")@y.values[[1]]
waus_acc_auc <- rbind(waus_acc_auc,
                      data.frame(model = "Gradient-boosted trees",
                                 accuracy = waus_lgbm_acc, auc = waus_lgbm_auc))
```

ROC curves at Queestion 10.

```{r echo = FALSE, fig.height = 5, fig.width = 7}
plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)
plot(waus_lgbm_perf, col = "green", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree", "Gradient-boosted trees"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue", "green"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")
```

Code to make predictions, create confusion matrix, report accuracy, construct ROC curve, compute AUC value and update classifier comparison table for the artificial neural network in Question 11.

```{r eval = FALSE}
waus_nn <- neuralnet(MHT == 1 ~ ., waus_train_nn, hidden = 3, linear.output = FALSE)
waus_nn_predict <- predict(waus_nn, waus_test_nn[, 1:17])
waus_nn_cm <- table("Predicted Class" = ifelse(waus_nn_predict > 0.5, 1, 0),
                    "Actual Class" = waus_test_nn$MHT)
waus_nn_acc <- get_accuracy(waus_nn_cm)
waus_nn_pred <- prediction(waus_nn_predict, waus_test_nn$MHT)
waus_nn_perf <- performance(waus_nn_pred, "tpr", "fpr")

plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)
plot(waus_lgbm_perf, col = "green", add = TRUE)
plot(waus_nn_perf, col = "chocolate", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree", "Gradient-boosted trees", "ANN"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue", "green", "chocolate"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")

waus_nn_auc <- performance(waus_nn_pred, "auc")@y.values[[1]]
waus_acc_auc <- rbind(waus_acc_auc,
                      data.frame(model = "Artificial neural network",
                                 accuracy = waus_nn_acc, auc = waus_nn_auc))
```

ROC curves at Queestion 11.

```{r echo = FALSE, fig.height = 5, fig.width = 7}
plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)
plot(waus_lgbm_perf, col = "green", add = TRUE)
plot(waus_nn_perf, col = "chocolate", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree", "Gradient-boosted trees", "ANN"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue", "green",
               "chocolate"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")
```

Code to make predictions, create confusion matrix, report accuracy, construct ROC curve, compute AUC value and update classifier comparison table for the new classifier (support vector machine) in Question 12.

```{r eval = FALSE}
waus_svm_predict <- predict(waus_svm, waus_test)
waus_svm_cm <- table("Predicted Class" = waus_svm_predict,
                     "Actual Class" = waus_test$MHT)
waus_svm_acc <- get_accuracy(waus_svm_cm)

waus_svm_prob <- ksvm(MHT ~ ., data = waus_train, kernel = "vanilladot",
                      prob.model = TRUE)
waus_svm_predict_prob <- predict(waus_svm_prob, waus_test, type = "prob")
waus_svm_pred <- prediction(waus_svm_predict_prob[, 2], waus_test$MHT)
waus_svm_perf <- performance(waus_svm_pred, "tpr", "fpr")

plot(waus_tree_perf, col = "red")
plot(waus_bayes_perf, col = "blue", add = TRUE)
plot(waus_bag_perf, col = "darkgreen", add = TRUE)
plot(waus_boost_perf, col = "violet", add = TRUE)
plot(waus_forest_perf, col = "gold", add = TRUE)
plot(waus_tree_pruned_perf, col = "skyblue", add = TRUE)
plot(waus_lgbm_perf, col = "green", add = TRUE)
plot(waus_nn_perf, col = "chocolate", add = TRUE)
plot(waus_svm_perf, col = "grey", add = TRUE)

abline(0, 1)
legend("bottomright",
       c("Decision tree", "Naive Bayes", "Bagging", "Boosting", "Random forest",
         "Pruned decision tree", "Gradient-boosted trees", "ANN", "SVM"),
       col = c("red", "blue", "darkgreen", "violet", "gold", "skyblue", "green",
               "chocolate", "grey"),
       lty = 1, bty = "n", inset = c(0, 0))
title("ROC curves for classifiers that predict MHT")

waus_svm_auc <- performance(waus_svm_pred, "auc")@y.values[[1]]
waus_acc_auc <- rbind(waus_acc_auc, data.frame(model = "Support vector machine",
                                               accuracy = waus_svm_acc,
                                               auc = waus_svm_auc))
```