---
urlcolor: blue
output: pdf_document
---
# FIT3152 Data analytics

## Assignment 3

**Name:** Rhyme Bulbul

**Student ID:** 31865224

**AI statement:** 



## Task 1
I gathered a collection of 19 text documents from different sources that covered a range of themes for this task. Blogs, news stories, movie reviews, and more are included in these publications. The Appendix contains the list of references for these documents.

Text files were created by copying and pasting the document contents.


## Task 2
Task 1 already had the document's contents pasted into text files. The text files were assembled into the `text` folder within the working directory in order to generate the corpus. Next, the code that follows is executed. It installs and imports the necessary libraries, sets the seed and uses the `Corpus()` function from the `tm` package to construct the corpus.

```{r message = FALSE}
rm(list = ls())
set.seed(31865224)
#install.packages("tm")
library(tm)
#install.packages("cluster")
library(cluster)
#install.packages("igraph")
library(igraph)

cname <- file.path(".", "text")
docs <- Corpus(DirSource((cname)))
list.files("text")
```

Documents are named based on the topic, suffixed by a number. Such as, `linux01.txt` is the first text document on linux systems, followed by `linux02.txt` the second text document on the same topic, and so fourth.


## Task 3

We start by text transforming the corpus, meanwhile replacing dashes and line breaks with spaces for consistency, removing numbers, punctuation, converting all characters to lowercase, and removing any extra white spaces. In addition, we also remove English stopwords before finally stemming all words for consistency. This is required as we don't want these characters creating an unwanted bias in our data, as it would be harder to work with, as well as inaccurate.

```{r}
to_space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, to_space, "-")
docs <- tm_map(docs, to_space, "\n")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument, language = "english")
```

With our unwanted terms removed, and desired keywords preserverd, the corpus is now ready to create a document term matrix from.

```{r}
dtm <- DocumentTermMatrix(docs)
```

Let's analyse the attributes of the document term matrix by inspecting a sample of 20 from it's head and tail of most and least frequent terms in alphabetical order.

```{r}
inspect(dtm)
```

```{r}
freq <- colSums(as.matrix(dtm))
freq[head(order(freq), 20)]
```

```{r}
freq[tail(order(freq), 20)]
```

With 6062 terms, or as we call it, tokens; the DTM is highly sparse at 88%. Interestingly, the least occurring tokens seem to only appear once, while the most frequent token is `android`, occurring almost 500 times, although it is only expected to be used in 3 documents, pointing to it's sparsity.

This leads us to the removal of sparse tokens from the document term matrix. This time we set the sparsity to 0.7 as it gives us the best mix of efficiency, reliability, and observability.

<!-- The next step is to remove sparse terms from the DTM. The maximal allowed sparsity, `sparse`, is set to 0.65, which gives a DTM with 54% sparsity and 32 tokens. A DTM closer to the guideline with 51% sparsity and 23 tokens can be obtained by setting `sparse` to 0.6, but through my analysis I observed that the 32-token DTM gives a better cluster dendrogram and network graph. **I also discovered that setting the sparsity even higher further improves the clustering, but I avoided this due to efficiency issues and poor readability of graphs.** -->

```{r}
dtms <- removeSparseTerms(dtm, 0.15)
inspect(dtms)
freqs <- colSums(as.matrix(dtms))
freqs[head(order(freqs), 20)]
freqs[tail(order(freqs), 20)]
```

This time round, the least frequent tokens are `exact`, `speed`, `pass` and `care` while the most frequent is `use` with 259 uses. The Document term matrix in it's full length can be found attached in the appendix.


## Task 4

<!-- A hierarchical clustering of the corpus is done based on two metrics, Euclidean distance and cosine distance, to see which performs better. The DTM is first converted to a standard matrix format. -->

```{r}
dtms_matrix <- as.matrix(dtms)
```

<!-- In document clustering, each document is represented as a vector with many dimensions that correspond to terms it contains. Euclidean distance measures the straight-line distance between these vectors, grouping them based on closeness. A smaller distance represents higher similarity. Cosine distance measures the angle between vectors, with smaller angles corresponding to closer distance and higher similarity. Clustering with cosine distance can be further improved by weighting the DTM with the term frequency-inverse document frequency (TF-IDF) statistic beforehand, which assigns higher weights to terms that appear frequently within a document (implying importance), but rarely across all documents (implying significance). -->

<!-- The code for clustering using Euclidean distance is adapted from Lecture 10. `dtms_matrix` is scaled and converted to a Euclidean distance matrix, and a dendrogram is plotted. -->

```{r fig.height = 5, fig.width = 7}
dist_euclid <- dist(scale(dtms_matrix))
fit_euclid <- hclust(dist_euclid, method = "ward.D")
plot(fit_euclid, hang = -1)
```

<!-- To cluster with TF-IDF weighting and cosine distance, the IDF for each term is first computed and a TF-IDF weighted matrix is obtained by applying the cross product of the TFs and IDFs. The formula for cosine distance is then applied to the matrix to get a cosine distance matrix. The dendrogram is then plotted. The code for this was adapted from [this Stack Overflow thread](https://stackoverflow.com/questions/52391558/hierarchical-clustering-using-cosine-distance-in-r). The code was studied and understood before application. -->

```{r fig.height = 5, fig.width = 7}
idf <- log(ncol(dtms_matrix) / (1 + rowSums(dtms_matrix != 0)))
idf <- diag(idf)
dtms_matrix_tfidf <- crossprod(dtms_matrix, idf)
colnames(dtms_matrix_tfidf) <- rownames(dtms_matrix)

dist_cos <- 1 - crossprod(dtms_matrix_tfidf) / (sqrt(colSums(dtms_matrix_tfidf ** 2) %*%
            t(colSums(dtms_matrix_tfidf ** 2))))
dist_cos[is.na(dist_cos)] <- 0
dist_cos <- as.dist(dist_cos)

fit_cos <- hclust(dist_cos, method = "ward.D")
plot(fit_cos, hang = -1)
```

<!-- At the highest level in Euclidean distance clustering, `linux02` is in a single cluster while all other documents are grouped into another cluster. This indicates that the clustering algorithm identifies `linux02` as an outlier document with distinct characteristics. However, it does share a common topic with `linux01`, so this is an inaccurate and imbalanced result. For the rest of the documents, some documents with the same topics are clustered together at low levels, such as `churchill01` and `churchill02`. However, there are some prominent inaccuracies, such as `bitcoin01` and `bitcoin02` being clustered far from each other (only grouped together at the highest level) and `asioaf02` (an extract from a fantasy novel) being clustered together with `stats01` (an extract from a statistics textbook) at the lowest level. -->

<!-- With cosine distance, the clustering produces more balanced clusters. `linux02` is grouped with several other documents at the highest level, but it is still separated from `linux01`. However, this clustering is overall better at grouping documents of the same topic together. For example, `bitcoin01` and `bitcoin02` are closer together, while the pairs of `covid01`-`covid02` and `yeats01`-`yeats03` are each in their own clusters at the lowest level. The overall height values of the clustering are also much smaller (with Euclidean distance, the largest height was 28.6 compared to 2.28 for cosine distance), indicating higher confidence in the grouping of similar documents. Conclusively, this is a better clustering for the documents compared to that with Euclidean distance. -->

<!-- Since the actual topic of each document is known, it is possible to get a quantitative measure of each clustering by labelling each document with its topic, plotting a confusion matrix of the clustering, and computing the accuracy. Due to length, the confusion matrices and the cluster-topic assignments are shown in the Appendix (for Euclidean distance, 15 clusters are created to reduce topic-cluster ambiguity). -->

```{r results = "hide"}
# function to remove suffix in document filename
short_name <- function(doc) {
    return(substr(doc, 1, nchar(doc) - 6))
}

doc_names <- list.files("docs")
doc_names_short <- unlist

# table(Topic = doc_names_short, Cluster = cutree(fit_euclid, k = 10))
# table(Topic = doc_names_short, Cluster = cutree(fit_cos, k = 10))
```

<!-- The accuracies of each matrix is calculated by hand. For clustering with Euclidean distance, this is given by -->

```{r}
13 / 23
```

<!-- Whereas for cosine distance, this is given by -->

```{r}
15 / 23
```

<!-- In agreement with the observations, clustering with cosine distance has a significantly higher accuracy than clustering with Euclidean distance. -->

<!-- Hierarchical clustering can be said to be the quicker and easier way to get an understanding of the relationships between documents and tokens in the corpus. It neatly groups documents into clusters at different heights, so groupings can be interpreted based on the desired cluster size. The accuracies of the clusterings performed in Task 4 are not very high, but an accuracy of around 0.65 for clustering with cosine distance can be considered strong for a small corpus (only 23 documents) and the genericness of the tokens. -->

<!-- However, clustering does not identify important documents, tokens or groups. Social network analysis visualises relationships between documents, tokens and document-token pairs, allowing viewers to quickly gain insight about the relationships within the corpus. If needed, viewers can access computable metrics (eg. closeness, betweenness, transitivity, etc.) to gauge the overall connectedness of the network or the importance of each document/token. This makes social networks a flexible way to identify important groups and relationships in the data that can be utilised by more viewers, be they a general audience or a technical one. -->

## Task 5

<!-- To create a single-mode network visualising the connections between documents based on the number of shared terms, the DTM is first converted to a binary matrix and then multiplied by its transpose. After the first step, a matrix which records 1 for each token that appears in that row's document is produced. After multiplication, the resulting matrix shows the number of shared tokens in each pair of documents. After getting this abstracts matrix, `graph_from_adjacency_matrix()` is used to plot the graph. -->

```{r fig.height = 7, fig.width = 7}
dtms_mat_bin <- as.matrix((dtms_matrix > 0) + 0)
abs_mat <- dtms_mat_bin %*% t(dtms_mat_bin)
diag(abs_mat) <- 0

abs_net <- graph_from_adjacency_matrix(abs_mat, mode = "undirected", weighted = TRUE)
plot(abs_net)
```

<!-- The graph's high density can be immediately observed; almost every possible pair of vertices have an edge. Computing the density of the graph proves this. -->

```{r}
graph.density(abs_net)
```

<!-- This shows that, based on shared terms, each document is related to almost every other document to some extent. `linux02` and `asioaf03` are very close to each other compared to other vertex pairs, indicating a strong relationship, while `yeats03` seems to have relatively weaker connections to the other documents. -->

<!-- To identify clear groups in the data, a good start is to get the transitivity of the graph. -->

```{r}
transitivity(abs_net)
```

<!-- Transitivity, or clustering coefficient, is the proportion of triangles relative to the number of connected triples, a higher value of which indicates more tightly connected groups. The transitivity of this graph is very high at 0.992. The distribution of the documents into groups can be done in many ways, as in hierarchical clustering, but a more prominent group might be `yeats03`, `yeats02`, `hamlet02` and `hamlet01`, all of which are literary texts. -->

<!-- At first glance, `asioaf03` and `linux02` seem to be the most important documents due to their central positions. To get numerical measurements, the following code is run. Due to length, the outputs are shown in the Appendix. -->

```{r results = "hide"}
sort(-closeness(abs_net))
sort(-betweenness(abs_net))
sort(evcent(abs_net)$vector)
sort(degree(abs_net))
```

<!-- Closeness measures how well a document is connected to others, betweenness measures the document's potential to be an intermediary (and thus have more influence over the network flow), eigenvector centrality measures the quality of connections of a document, and degree is a vertex's number of connections. For closeness and betweenness, the results are made negative as the weights of the vertices correspond to the number of shared terms, meaning when two documents are closely connected, the distance (weight of the edge) between them is high.  -->

<!-- For betweenness, 13 documents have values of 0, indicating that they all have maximum potential of being the network's "hubs". For degree, only `yeats03`, `bitcoin01` and `stats02` are not connected to every other document. However, this is not a strong indicator that they are less important, as their degrees are still very close to the maximum of 22. `yeats03` has the smallest value for all four metrics, so there is confidence that it is the least important document. On the contrary, `linux02` has the highest closeness and eigenvector centralities, a sign that it is possibly the most important document. -->

<!-- To visualise the interesting features of the data, the graph is improved by colouring the vertices based on topic, scaling the width of edges based on inter-document strength (number of shared terms) and scaling the vertex sizes based on the document's closeness centrality. The code to do this is given as follows. -->

```{r fig.height = 7, fig.width = 7}
doc_colr <- c("red", "green", "dodgerblue", "yellow", "orange", "magenta", "pink",
              "chocolate", "gray", "cyan")
# topics <- unique(doc_names_short)

for (doc in doc_names) {
    V(abs_net)[doc]$color <- doc_colr[match(short_name(doc), topics)]
}

V(abs_net)$size <- 1 / closeness(abs_net, mode = "all") / 10
E(abs_net)$width <- E(abs_net)$weight / 6

plot(abs_net)
# legend(x = -1.5, y = -0.5, legend = topics, pch = 21, cex = 1,
#        pt.bg = doc_colr, bty = "n", ncol = 1)
```

<!-- With this graph, we can more clearly understand the distance between the `yeats` documents and the others. It is also easier to observe that some topics have all documents close together (eg. `asioaf`) while others have the opposite (eg. `hamlet`). -->


## Task 6

<!-- To create a single-mode network to illustrate the connections between the tokens based on the number of common documents they appear in, most of the code is reused from Task 5 with some adjustments. To create the tokens matrix, the transpose of the binary matrix is multiplied by the original. The graph is then plotted with the same function. -->

```{r fig.height = 7, fig.width = 7}
tok_mat <- t(dtms_mat_bin) %*% dtms_mat_bin
diag(tok_mat) <- 0

tok_net <- graph_from_adjacency_matrix(tok_mat, mode = "undirected", weighted = TRUE)
plot(tok_net)
```

<!-- This graph looks denser than the one for documents. -->

```{r}
graph.density(tok_net)
```

<!-- In line with the observation, the graph is indeed denser with a density of 1, meaning all vertices are interconnected. This tells us that the tokens are quite generic words, appearing across all documents of various topics. -->

<!-- Looking at the graph, it is difficult to identify clear groups as the vertices seem quite evenly spread out. A suspected cluster is `time`, `like` and `just` at the top of the graph, which forms a noticeable triangle. -->

```{r}
transitivity(tok_net)
```

<!-- The transitivity of the graph is 1. Even though it is hard to distinguish clusters by observation, this metric tells us that every three tokens can form a triangle, indicating a very high number of potential clusters. This is possibly also due to the genericness of the tokens. **I have experimented with manually removing some generic words during DTM pre-processing, but it lead to poorer clustering performance.** This is probably because the generic terms, despite being common, provide important contextual information that links documents together. -->

<!-- The visually centre-most tokens are `can` and `one`, a sign that they may be relatively important. The following code is run to investigate this. The outputs are shown in the Appendix. -->

```{r results = "hide"}
sort(-closeness(tok_net))
sort(-betweenness(tok_net))
sort(evcent(tok_net)$vector)
sort(degree(tok_net))
```

<!-- `can` has the second highest closeness centrality behind `make`, maximum betweenness centrality, the second highest eigenvector centrality also behind `make`, and the maximum degree. `one` performs worse than expected; it ranks 5th in closeness and 6th in eigenvector centrality, but still has maximum possible betweenness. Looking at the `make` vertex in hindsight, it is noticeable now that the edges surrounding this vertex are more layered and dense. -->

<!-- To improve this graph, the vertex sizes are scaled to their closeness centralities, and the edge widths are scaled to their weights (number of shared documents between tokens). -->

```{r fig.height = 7, fig.width = 7}
V(tok_net)$size <- 1 / closeness(tok_net, mode = "all") / 10
E(tok_net)$width <- E(tok_net)$weight / 5

plot(tok_net)
```

<!-- With this graph, it can now be clearly seen that `make` and `can` are two important central tokens, while `man` is notably further away from the rest and, as justified by the previously computed metrics, is relatively less important. -->

## Task 7

<!-- To create a bipartite (two-mode) network graph, the data is first formatted using code adapted from Lecture 12. -->

```{r}
dtms_dfa <- as.data.frame(dtms_matrix)
dtms_dfa$abstract <- rownames(dtms_dfa)
dtms_dfb <- data.frame()
for (i in seq_len(nrow(dtms_dfa))) {
    for (j in seq_len(ncol(dtms_dfa) - 1)) {
        to_use <- cbind(dtms_dfa[i, j], dtms_dfa[i, ncol(dtms_dfa)],
                        colnames(dtms_dfa[j]))
        dtms_dfb <- rbind(dtms_dfb, to_use)
    }
}
colnames(dtms_dfb) <- c("weight", "abstract", "token")

dtms_dfc <- dtms_dfb[dtms_dfb$weight != 0, ]
dtms_dfc <- dtms_dfc[, c("abstract", "token", "weight")]
```

<!-- In the above code, a data frame showing the frequency (weight) of each token in each document (`dtms_dfb`) is created. Rows with weights of 0 are removed in a new data frame (`dtms_dfc`) and this data frame is used to plot the bipartite graph. -->

```{r fig.height = 8, fig.width = 8}
bipart <- graph.data.frame(dtms_dfc, directed = FALSE)
V(bipart)$type <- bipartite_mapping(bipart)$type
V(bipart)$color <- ifelse(V(bipart)$type, "pink", "green")
V(bipart)$shape <- ifelse(V(bipart)$type, "square", "circle")
E(bipart)$color <- "grey"

plot(bipart)
```

<!-- As this graph is bipartite, the density and transitivity are expected to be very low. The closeness, betweenness and eigenvector centralities do not give us new insight, as the relationship between documents and that between tokens have already been analysed in Tasks 5 and 6. Hence, this graph is analysed by observation. -->

<!-- The overall structure of this graph resembles a "cluster" of documents in the middle surrounded by the tokens in a ring, all of which are surrounded by the remaining documents. The central cluster of documents are positioned as such as they are linked to most of the tokens. This cluster includes `linux02`, `asioaf03` and `covid03`, which the metrics computed in Task 5 determined to be the relatively more important documents. Similar to the graph in Task 5, `yeats03` lies relatively further away from the other vertices, with links to fewer tokens. The degrees of the vertices prove this (output shown in Appendix). -->

```{r results = "hide"}
sort(degree(bipart))
```

<!-- `linux02` and `covid03` have the two highest degrees while `yeats03` has the least. -->

<!-- Other notable groups of documents which are not within the central cluster include the group of all three `stats` documents, which are situated close together on the left of the graph, and the pair of `asoiaf02` and `covid02` at the top, which are separated from the other documents by a "wall" of tokens. -->

<!-- To improve this graph, document vertices are made smaller for readability and coloured as in Task 5, token vertices are scaled according to their degree and re-coloured white, edges are coloured based on the document vertex it connects to, and edge widths are scaled according to the weight of the edge (frequency of token in document). Document vertex sizes are not scaled as the observable amount of coloured outgoing edges gives a good idea of their degrees.  -->

```{r fig.height = 8, fig.width = 8}
for (i in seq_len(23)) V(bipart)$size[i] <- 10
V(bipart)$size[24:55] <- degree(bipart)[24:55]
for (j in seq_len(length(doc_names))) {
    V(bipart)[j]$color <- doc_colr[match(short_name(doc_names[j]), topics)]
}
for (k in c(24:length(V(bipart)))) V(bipart)[k]$color <- "white"
E(bipart)$width <- as.numeric(dtms_dfc$weight) / 5
E(bipart)$color <- tail_of(bipart, E(bipart))$color

plot(bipart)
legend(x = -1.5, y = -0.5, legend = c(topics, "token"), pch = 21, cex = 1,
       pt.bg = c(doc_colr, "white"), bty = "n", ncol = 1)
```

<!-- From this improved graph, the high relative importance of `linux02` is more clearly seen, having edges that are significantly thicker than its close neighbour `bitcoin02`, which is considered to be of high importance as well. `make`, `can` and `will` form a triple of tokens that appear the most across all documents. Some documents, such as `churchill01`, are within the central cluster despite having thin edges, indicating that they contain smaller numbers of many tokens. This contrasts with outer documents such as the `yeats` documents, which have thick edges but not that many edges in total, indicating a larger numbers of fewer tokens. -->














TODO: Have approx 20 tokens after removing sparse terms
Remove name suffixes
generate euclid and cos distances









































## Appendix


## References

Wikimedia Foundation. (2024, May 14). Ubuntu. Wikipedia. https://en.wikipedia.org/wiki/Ubuntu 

Wikimedia Foundation. (2024c, May 18). Linux kernel. Wikipedia. https://en.wikipedia.org/wiki/Linux_kernel 

Wikimedia Foundation. (2024a, April 5). Lineageos. Wikipedia. https://en.wikipedia.org/wiki/LineageOS 

Wikimedia Foundation. (2024b, May 7). Android (Operating System). Wikipedia. https://en.wikipedia.org/wiki/Android_(operating_system) 

Encyclopædia Britannica, inc. (2024, May 14). Australia. Encyclopædia Britannica. https://www.britannica.com/place/Australia 

Goode, L. (2024, May 14). It’s the end of google search as we know it. Wired. https://www.wired.com/story/google-io-end-of-google-search/ 

IMDb.com. (2003, June 6). 2 fast 2 furious. IMDb. https://www.imdb.com/title/tt0322259/?ref_=nv_sr_srsg_0_tt_7_nm_1_q_2%2520fast 

IMDb.com. (2009, April 3). Fast & Furious. IMDb. https://www.imdb.com/title/tt1013752/?ref_=nv_sr_srsg_1_tt_7_nm_0_q_fast%2520and%2520fur 

IMDb.com. (2003b, July 9). Pirates of the Caribbean: The curse of the black pearl. IMDb. https://www.imdb.com/title/tt0325980/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_pirates 

IMDb.com. (2017, May 26). Pirates of the caribbean: Dead men tell no tales. IMDb. https://www.imdb.com/title/tt1790809/?ref_=nv_sr_srsg_3_tt_8_nm_0_q_pirates 

IMDb.com. (2014, October 24). John Wick. IMDb. https://www.imdb.com/title/tt2911666/?ref_=nv_sr_srsg_1_tt_7_nm_1_q_jon%2520wick 

IMDb.com. (1996, May 22). Mission: Impossible. IMDb. https://www.imdb.com/title/tt0117060/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_mission 

IMDb.com. (2016, February 12). Deadpool. IMDb. https://www.imdb.com/title/tt1431045/?ref_=nv_sr_srsg_3_tt_6_nm_2_q_deadpool 

IMDb.com. (1984, October 26). The terminator. IMDb. https://www.imdb.com/title/tt0088247/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_terminator 

IMDb.com. (1977, May 25). Star wars: Episode IV - A new hope. IMDb. https://www.imdb.com/title/tt0076759/?ref_=nv_sr_srsg_4_tt_7_nm_0_q_star%2520wars 

IMDb.com. (2004, May 19). Shrek 2. IMDb. https://www.imdb.com/title/tt0298148/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_shrek%25202 

IMDb.com. (2006, October 20). The prestige. IMDb. https://www.imdb.com/title/tt0482571/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_prestige 

IMDb.com. (2013, December 25). The wolf of wall street. IMDb. https://www.imdb.com/title/tt0993846/?ref_=nm_flmg_t_38_prd 

IMDb.com. (2002, December 25). Catch me if you can. IMDb. https://www.imdb.com/title/tt0264464/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_catch%2520me 




