---
urlcolor: blue
output: pdf_document
---
# FIT3152 Data analytics

## Assignment 3

**Name:** Rhyme Bulbul

**Student ID:** 31865224

**AI statement:** Artificial Intelligence was not used in this report



## Task 1
I gathered a collection of 19 text documents from different sources that covered a range of themes for this task. Blogs, news stories, movie reviews, and more are included in these publications. The Appendix contains the list of references for these documents.

Text files were created by copying and pasting the document contents.



## Task 2
Task 1 already had the document's contents pasted into text files. The text files were assembled into the `text` folder within the working directory in order to generate the corpus. Next, the code that follows is executed. It installs and imports the necessary libraries, sets the seed and uses the `Corpus()` function from the `tm` package to construct the corpus.

```{r message = FALSE}
rm(list = ls())
set.seed(31865224)

library(tm)
library(cluster)
library(igraph)

cname <- file.path(".", "text")
docs <- Corpus(DirSource((cname)))
list.files("text")
```

Documents are named based on the topic, suffixed by a number. Such as, `linux01.txt` is the first text document on linux systems, followed by `linux02.txt` the second text document on the same topic, and so fourth.



## Task 3

We start by text transforming the corpus, meanwhile replacing dashes and line breaks with spaces for consistency, removing numbers, punctuation, converting all characters to lowercase, and removing any extra white spaces. In addition, we also remove English stop words before finally stemming all words for consistency. This is required as we don't want these characters creating an unwanted bias in our data, as it would be harder to work with, as well as inaccurate.

```{r}
to_space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, to_space, "-")
docs <- tm_map(docs, to_space, "\n")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument, language = "english")
```

With our unwanted terms removed, and desired keywords preserverd, the corpus is now ready to create a document term matrix from.

```{r}
dtm <- DocumentTermMatrix(docs)
```

Let's analyse the attributes of the document term matrix by inspecting a sample of 20 from it's head and tail of most and least frequent terms in alphabetical order.

```{r}
inspect(dtm)
```

```{r}
freq <- colSums(as.matrix(dtm))
freq[head(order(freq), 20)]
```

```{r}
freq[tail(order(freq), 20)]
```

With 6062 terms, or as we call it, tokens; the DTM is highly sparse at 88%. Interestingly, the least occurring tokens seem to only appear once, while the most frequent token is `android`, occurring almost 500 times, although it is only expected to be used in 3 documents, pointing to it's sparsity.

This leads us to the removal of sparse tokens from the document term matrix. This time we set the sparsity to 9% as it gives us the best mix of efficiency, reliability, and observability. This figure was chosen as it allows the DTM to have around 20 tokens after removing sparse terms without sacrificing on other attributes.

```{r}
dtms <- removeSparseTerms(dtm, 0.09)
inspect(dtms)
freqs <- colSums(as.matrix(dtms))
freqs[head(order(freqs), 20)]
freqs[tail(order(freqs), 20)]
```

This time round, the least frequent tokens are `exact`, `speed`, `pass` and `care` while the most frequent is `use` with 259 uses. The Document term matrix in it's full length can be found attached in the appendix.



## Task 4

To determine which performs better, a hierarchical clustering of the corpus is carried out using two metrics: Euclidean distance and cosine distance. First, the DTM is transformed into a regular matrix format.

```{r}
dtms_matrix <- as.matrix(dtms)
```

Each document in document clustering is represented as a vector with numerous dimensions that match the terms it contains. Euclidean distance classifies these vectors according to proximity by measuring the straight-line distance between them. Greater similarity is indicated by a shorter distance. The angle between vectors is measured by cosine distance; smaller angles indicate closer distances and more similarity. By pre-weighting the DTM with the term frequency-inverse document frequency (TF-IDF) statistic, which gives higher weights to terms that appear frequently within a document (implying importance) but infrequently across all documents (implying significance), clustering with cosine distance can be further enhanced.

This code is an adaptation of Lecture 10's clustering using Euclidean distance algorithm. After scaling and converting `dtms_matrix` to a Euclidean distance matrix, a dendrogram is shown.

```{r fig.height = 5, fig.width = 7}
dist_euclid <- dist(scale(dtms_matrix))
fit_euclid <- hclust(dist_euclid, method = "ward.D")
plot(fit_euclid, hang = -1)
```

The IDF for each term is first calculated, and a TF-IDF weighted matrix is generated by applying the cross product of the TFs and IDFs in order to cluster with TF-IDF weighting and cosine distance. Next, the matrix is subjected to the cosine distance formula to obtain a cosine distance matrix. After that, the dendrogram is plotted.

```{r fig.height = 5, fig.width = 7}
idf <- log(ncol(dtms_matrix) / (1 + rowSums(dtms_matrix != 0)))
idf <- diag(idf)
dtms_matrix_tfidf <- crossprod(dtms_matrix, idf)
colnames(dtms_matrix_tfidf) <- rownames(dtms_matrix)

dist_cos <- 1 - crossprod(dtms_matrix_tfidf) / (sqrt(colSums(dtms_matrix_tfidf ** 2) %*%
            t(colSums(dtms_matrix_tfidf ** 2))))
dist_cos[is.na(dist_cos)] <- 0
dist_cos <- as.dist(dist_cos)

fit_cos <- hclust(dist_cos, method = "ward.D")
plot(fit_cos, hang = -1)
```

In terms of Euclidean distance clustering, the first divide is between two clusters, one of movie reviews including `fast`, `pirates` and `impossible`, and another of purely `linux`. This is pretty accurate, with the `linux` cluster branching off with a new document in each cluster. The movies reviews cluster is further branched off with `fast02` seperated from all others. While unexpected, this points the document to be an outlier. The next two clusters branched do a worse job, with all movies reviews being split accross the clusters, it also noted that with heights much lower than 20, we expect lower accuracy compared to the `linux` cluster.

Cosine Distance clustering is similar story, however only correctly identifying three `linux` documents into a cluster, and all the others into another. With an even lower height, less than one, for this cluster this time, even for the `linux` cluster, it is no surprise that the movies reviews cluster is highly inaccurate, with no real correlation between clustered documents.

By marking each document with its topic, creating a confusion matrix of the clustering, and calculating the accuracy, it is able to obtain a quantitative assessment of each clustering because each document's true topic is known. Owing to their length, the Appendix displays the confusion matrices and the cluster-topic assignments, and in order to reduce topic-cluster ambiguity for Euclidean distance, 15 clusters are produced.

```{r results = "hide"}
short_name <- function(doc) {
    return(substr(doc, 1, nchar(doc) - 6))
}

doc_names <- list.files("text")
doc_names_short <- unlist(lapply(doc_names, short_name))

table(Topic = doc_names_short, Cluster = cutree(fit_euclid, k = 15))
table(Topic = doc_names_short, Cluster = cutree(fit_cos, k = 10))
```

The matrix accuracy is determined by hand. This is provided for clustering using Euclidean distance by

```{r}
13 / 20
```

And for Cosine distance is provided by

```{r}
15 / 20
```

Clustering with cosine distance has a considerably higher accuracy than clustering with Euclidean distance, which is consistent with the data.

Hierarchical clustering can be said to be the quicker and easier way to get an understanding of the relationships between documents and tokens in the corpus. It neatly groups documents into clusters at different heights, so groupings can be interpreted based on the desired cluster size. The accuracies of the clusterings performed in Task 4 are not very high, but an accuracy of around 0.65 for clustering with cosine distance can be considered strong for a small corpus (only 20 documents) and the genericness of the tokens.

Clustering, however, is unable to recognise significant documents, tokens, or groupings. Social network analysis helps users quickly understand the relationships within the corpus by visualising the relationships between documents, tokens, and document-token pairs. Viewers can use computed metrics (such as proximity, betweenness, transitivity, etc.) to determine the significance of each document or token or the network's overall connectivity if necessary. Because of this, social networks provide a versatile means of locating significant clusters and connections within the data that may be leveraged by a larger number of users, whether they be technical or public viewers.



## Task 5

The DTM is first transformed into a binary matrix and then multiplied by its transpose in order to construct a single-mode network that visualises the connections between documents based on the quantity of shared phrases. Following the first phase, a matrix is created that has a record of 1 for each token that appears in the document of that row. Following multiplication, the number of shared tokens in every pair of documents is displayed in the resulting matrix.

```{r fig.height = 7, fig.width = 7}
dtms_mat_bin <- as.matrix((dtms_matrix > 0) + 0)
abs_mat <- dtms_mat_bin %*% t(dtms_mat_bin)
diag(abs_mat) <- 0

abs_net <- graph_from_adjacency_matrix(abs_mat, mode = "undirected", weighted = TRUE)
plot(abs_net)
```

It is easy to see that the graph is quite dense; practically every pair of vertices that might exist has an edge. This is demonstrated by computing the graph's density.

```{r}
graph.density(abs_net)
```

This demonstrates how each document is connected to nearly every other document to some degree based on common phrases. In comparison to other vertex pairings, `pirates03`, and `pirates01` are extremely close to `fast02`, suggesting a strong association, whereas `linux05` appears to have comparatively weaker ties to the other documents.

Finding the graph's transitivity is an excellent place to start when trying to find distinct groups in the data.

```{r}
transitivity(abs_net)
```

<!-- Transitivity, or clustering coefficient, is the proportion of triangles relative to the number of connected triples, a higher value of which indicates more tightly connected groups. The transitivity of this graph is very high at 0.992. The distribution of the documents into groups can be done in many ways, as in hierarchical clustering, but a more prominent group might be `yeats03`, `yeats02`, `hamlet02` and `hamlet01`, all of which are literary texts. -->

<!-- At first glance, `asioaf03` and `linux02` seem to be the most important documents due to their central positions. To get numerical measurements, the following code is run. Due to length, the outputs are shown in the Appendix. -->

```{r results = "hide"}
sort(-closeness(abs_net))
sort(-betweenness(abs_net))
sort(evcent(abs_net)$vector)
sort(degree(abs_net))
```

<!-- Closeness measures how well a document is connected to others, betweenness measures the document's potential to be an intermediary (and thus have more influence over the network flow), eigenvector centrality measures the quality of connections of a document, and degree is a vertex's number of connections. For closeness and betweenness, the results are made negative as the weights of the vertices correspond to the number of shared terms, meaning when two documents are closely connected, the distance (weight of the edge) between them is high.  -->

<!-- For betweenness, 13 documents have values of 0, indicating that they all have maximum potential of being the network's "hubs". For degree, only `yeats03`, `bitcoin01` and `stats02` are not connected to every other document. However, this is not a strong indicator that they are less important, as their degrees are still very close to the maximum of 22. `yeats03` has the smallest value for all four metrics, so there is confidence that it is the least important document. On the contrary, `linux02` has the highest closeness and eigenvector centralities, a sign that it is possibly the most important document. -->

<!-- To visualise the interesting features of the data, the graph is improved by colouring the vertices based on topic, scaling the width of edges based on inter-document strength (number of shared terms) and scaling the vertex sizes based on the document's closeness centrality. The code to do this is given as follows. -->

```{r fig.height = 7, fig.width = 7}
doc_colr <- c("red", "green", "dodgerblue", "yellow", "orange", "magenta", "pink",
              "chocolate", "gray", "cyan")
topics <- unique(doc_names_short)

for (doc in doc_names) {
    V(abs_net)[doc]$color <- doc_colr[match(short_name(doc), topics)]
}

V(abs_net)$size <- 1 / closeness(abs_net, mode = "all") / 20
E(abs_net)$width <- E(abs_net)$weight / 12

plot(abs_net)
legend(x = -1.5, y = -0.5, legend = topics, pch = 21, cex = 1,
       pt.bg = doc_colr, bty = "n", ncol = 1)
```

<!-- With this graph, we can more clearly understand the distance between the `yeats` documents and the others. It is also easier to observe that some topics have all documents close together (eg. `asioaf`) while others have the opposite (eg. `hamlet`). -->


## Task 6

<!-- To create a single-mode network to illustrate the connections between the tokens based on the number of common documents they appear in, most of the code is reused from Task 5 with some adjustments. To create the tokens matrix, the transpose of the binary matrix is multiplied by the original. The graph is then plotted with the same function. -->

```{r fig.height = 7, fig.width = 7}
tok_mat <- t(dtms_mat_bin) %*% dtms_mat_bin
diag(tok_mat) <- 0

tok_net <- graph_from_adjacency_matrix(tok_mat, mode = "undirected", weighted = TRUE)
plot(tok_net)
```

<!-- This graph looks denser than the one for documents. -->

```{r}
graph.density(tok_net)
```

<!-- In line with the observation, the graph is indeed denser with a density of 1, meaning all vertices are interconnected. This tells us that the tokens are quite generic words, appearing across all documents of various topics. -->

<!-- Looking at the graph, it is difficult to identify clear groups as the vertices seem quite evenly spread out. A suspected cluster is `time`, `like` and `just` at the top of the graph, which forms a noticeable triangle. -->

```{r}
transitivity(tok_net)
```

<!-- The transitivity of the graph is 1. Even though it is hard to distinguish clusters by observation, this metric tells us that every three tokens can form a triangle, indicating a very high number of potential clusters. This is possibly also due to the genericness of the tokens. **I have experimented with manually removing some generic words during DTM pre-processing, but it lead to poorer clustering performance.** This is probably because the generic terms, despite being common, provide important contextual information that links documents together. -->

<!-- The visually centre-most tokens are `can` and `one`, a sign that they may be relatively important. The following code is run to investigate this. The outputs are shown in the Appendix. -->

```{r results = "hide"}
sort(-closeness(tok_net))
sort(-betweenness(tok_net))
sort(evcent(tok_net)$vector)
sort(degree(tok_net))
```

<!-- `can` has the second highest closeness centrality behind `make`, maximum betweenness centrality, the second highest eigenvector centrality also behind `make`, and the maximum degree. `one` performs worse than expected; it ranks 5th in closeness and 6th in eigenvector centrality, but still has maximum possible betweenness. Looking at the `make` vertex in hindsight, it is noticeable now that the edges surrounding this vertex are more layered and dense. -->

<!-- To improve this graph, the vertex sizes are scaled to their closeness centralities, and the edge widths are scaled to their weights (number of shared documents between tokens). -->

```{r fig.height = 7, fig.width = 7}
V(tok_net)$size <- 1 / closeness(tok_net, mode = "all") / 20
E(tok_net)$width <- E(tok_net)$weight / 15

plot(tok_net)
```

<!-- With this graph, it can now be clearly seen that `make` and `can` are two important central tokens, while `man` is notably further away from the rest and, as justified by the previously computed metrics, is relatively less important. -->

## Task 7

<!-- To create a bipartite (two-mode) network graph, the data is first formatted using code adapted from Lecture 12. -->

```{r}
dtms_dfa <- as.data.frame(dtms_matrix)
dtms_dfa$abstract <- rownames(dtms_dfa)
dtms_dfb <- data.frame()
for (i in seq_len(nrow(dtms_dfa))) {
    for (j in seq_len(ncol(dtms_dfa) - 1)) {
        to_use <- cbind(dtms_dfa[i, j], dtms_dfa[i, ncol(dtms_dfa)],
                        colnames(dtms_dfa[j]))
        dtms_dfb <- rbind(dtms_dfb, to_use)
    }
}
colnames(dtms_dfb) <- c("weight", "abstract", "token")

dtms_dfc <- dtms_dfb[dtms_dfb$weight != 0, ]
dtms_dfc <- dtms_dfc[, c("abstract", "token", "weight")]
```

<!-- In the above code, a data frame showing the frequency (weight) of each token in each document (`dtms_dfb`) is created. Rows with weights of 0 are removed in a new data frame (`dtms_dfc`) and this data frame is used to plot the bipartite graph. -->

```{r fig.height = 8, fig.width = 8}
bipart <- graph.data.frame(dtms_dfc, directed = FALSE)
V(bipart)$type <- bipartite_mapping(bipart)$type
V(bipart)$color <- ifelse(V(bipart)$type, "pink", "green")
V(bipart)$shape <- ifelse(V(bipart)$type, "square", "circle")
E(bipart)$color <- "grey"

plot(bipart)
```

<!-- As this graph is bipartite, the density and transitivity are expected to be very low. The closeness, betweenness and eigenvector centralities do not give us new insight, as the relationship between documents and that between tokens have already been analysed in Tasks 5 and 6. Hence, this graph is analysed by observation. -->

<!-- The overall structure of this graph resembles a "cluster" of documents in the middle surrounded by the tokens in a ring, all of which are surrounded by the remaining documents. The central cluster of documents are positioned as such as they are linked to most of the tokens. This cluster includes `linux02`, `asioaf03` and `covid03`, which the metrics computed in Task 5 determined to be the relatively more important documents. Similar to the graph in Task 5, `yeats03` lies relatively further away from the other vertices, with links to fewer tokens. The degrees of the vertices prove this (output shown in Appendix). -->

```{r results = "hide"}
sort(degree(bipart))
```

<!-- `linux02` and `covid03` have the two highest degrees while `yeats03` has the least. -->

<!-- Other notable groups of documents which are not within the central cluster include the group of all three `stats` documents, which are situated close together on the left of the graph, and the pair of `asoiaf02` and `covid02` at the top, which are separated from the other documents by a "wall" of tokens. -->

<!-- To improve this graph, document vertices are made smaller for readability and coloured as in Task 5, token vertices are scaled according to their degree and re-coloured white, edges are coloured based on the document vertex it connects to, and edge widths are scaled according to the weight of the edge (frequency of token in document). Document vertex sizes are not scaled as the observable amount of coloured outgoing edges gives a good idea of their degrees.  -->

```{r fig.height = 8, fig.width = 8}
for (i in seq_len(23)) V(bipart)$size[i] <- 10
V(bipart)$size[20:50] <- degree(bipart)[20:50]
for (j in seq_len(length(doc_names))) {
    V(bipart)[j]$color <- doc_colr[match(short_name(doc_names[j]), topics)]
}
for (k in c(24:length(V(bipart)))) V(bipart)[k]$color <- "white"
E(bipart)$width <- as.numeric(dtms_dfc$weight) / 5
E(bipart)$color <- tail_of(bipart, E(bipart))$color

plot(bipart)
legend(x = -1.5, y = -0.5, legend = c(topics, "token"), pch = 21, cex = 1,
       pt.bg = c(doc_colr, "white"), bty = "n", ncol = 1)
```

<!-- From this improved graph, the high relative importance of `linux02` is more clearly seen, having edges that are significantly thicker than its close neighbour `bitcoin02`, which is considered to be of high importance as well. `make`, `can` and `will` form a triple of tokens that appear the most across all documents. Some documents, such as `churchill01`, are within the central cluster despite having thin edges, indicating that they contain smaller numbers of many tokens. This contrasts with outer documents such as the `yeats` documents, which have thick edges but not that many edges in total, indicating a larger numbers of fewer tokens. -->














TODO: 

4
Calculate euclid and cos matrix accuracies by hand

5
Adjust graph 2 parameters [Graph density]

6
Adjust graph 2 parameters [Graph density]

7
Adjust graph 2 bipart ratio parameters






































## Appendix


## References

Wikimedia Foundation. (2024, May 14). Ubuntu. Wikipedia. https://en.wikipedia.org/wiki/Ubuntu 

Wikimedia Foundation. (2024c, May 18). Linux kernel. Wikipedia. https://en.wikipedia.org/wiki/Linux_kernel 

Wikimedia Foundation. (2024a, April 5). Lineageos. Wikipedia. https://en.wikipedia.org/wiki/LineageOS 

Wikimedia Foundation. (2024b, May 7). Android (Operating System). Wikipedia. https://en.wikipedia.org/wiki/Android_(operating_system) 

Wikimedia Foundation. (2024b, May 7). Android (Operating System). Wikipedia. https://en.wikipedia.org/wiki/CyanogenMod

IMDb.com. (2003, June 6). 2 fast 2 furious. IMDb. https://www.imdb.com/title/tt0322259/?ref_=nv_sr_srsg_0_tt_7_nm_1_q_2%2520fast 

IMDb.com. (2009, April 3). Fast & Furious. IMDb. https://www.imdb.com/title/tt1013752/?ref_=nv_sr_srsg_1_tt_7_nm_0_q_fast%2520and%2520fur 

IMDb.com. (2006, June 16). The Fast and the Furious: Tokyo Drift. IMDb. https://www.imdb.com/title/tt0463985/?ref_=nv_sr_srsg_1_tt_7_nm_0_q_tokyo%2520d

IMDb.com. (2011, April 29). Fast five. IMDb. https://www.imdb.com/title/tt1596343/?ref_=fn_al_tt_6 

IMDb.com. (2013). Fast & Furious 6. IMDb. https://www.imdb.com/title/tt1905041/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_fast%25206

IMDb.com. (2003b, July 9). Pirates of the Caribbean: The curse of the black pearl. IMDb. https://www.imdb.com/title/tt0325980/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_pirates 

IMDb.com. (2006). Pirates of the Caribbean: Dead Man's Chest. IMdb. https://www.imdb.com/title/tt0383574/?ref_=nv_sr_srsg_6_tt_5_nm_3_q_pirates%2520of%2520the%2520carribean

IMDb.com. (2003b, July 9). Pirates of the Caribbean: At World's End. IMdb. https://www.imdb.com/title/tt0449088/?ref_=nv_sr_srsg_7_tt_5_nm_3_q_pirates%2520of%2520the%2520carribean

IMDb.com. (2003b, July 9). Pirates of the Caribbean: On Stranger Tides. IMdb. https://www.imdb.com/title/tt1298650/?ref_=nv_sr_srsg_9_tt_5_nm_3_q_pirates%2520of%2520the%2520carribean

IMDb.com. (2017, May 26). Pirates of the caribbean: Dead men tell no tales. IMDb. https://www.imdb.com/title/tt1790809/?ref_=nv_sr_srsg_3_tt_8_nm_0_q_pirates 

IMDb.com. (1996, May 22). Mission: Impossible. IMDb. https://www.imdb.com/title/tt0117060/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_mission 

IMDb.com. (2000). Mission: Impossible II. IMDb. https://www.imdb.com/title/tt0120755/?ref_=tt_sims_tt_t_1

IMDb.com. (2006). Mission: Impossible III. IMDb.  https://www.imdb.com/title/tt0317919/?ref_=tt_sims_tt_t_2

IMDb.com. (2011). Mission: Impossible - Ghost Protocol. IMDb.  https://www.imdb.com/title/tt1229238/?ref_=tt_sims_tt_t_3

IMDb.com. (2015). Mission: Impossible - Rogue Nation. IMDb. https://www.imdb.com/title/tt2381249/?ref_=tt_sims_tt_t_4





<!-- Document-term matrix at the end of Task 3, printed as a data frame. -->

```{r}
as.data.frame(as.matrix(dtms))
```

<!-- Confusion matrices used for quantitative measure of clustering in Task 4. -->

```{r}
table(Topic = doc_names_short, Cluster = cutree(fit_euclid, k = 10))
table(Topic = doc_names_short, Cluster = cutree(fit_cos, k = 10))
```

<!-- Euclidean distance cluster assignment for each topic, from Task 4. -->

<!-- - **1** `asoiaf` -->
<!-- - **2** `stats` -->
<!-- - **3** `NA` -->
<!-- - **4** `bitcoin` -->
<!-- - **5** `churchill` -->
<!-- - **6** `covid` -->
<!-- - **7** `NA` -->
<!-- - **8** `NA` -->
<!-- - **9** `hamlet` -->
<!-- - **10** `linux` -->
<!-- - **11** `NA` -->
<!-- - **12** `peaky` -->
<!-- - **13** `queen` -->
<!-- - **14** `yeats` -->
<!-- - **15** `NA` -->

<!-- Cosine distance cluster assignment for each topic, from Task 4. -->

<!-- - **1** `asioaf` -->
<!-- - **2** `queen` -->
<!-- - **3** `stats` -->
<!-- - **4** `bitcoin` -->
<!-- - **5** `churchill` -->
<!-- - **6** `covid` -->
<!-- - **7** `hamlet` -->
<!-- - **8** `peaky` -->
<!-- - **9** `linux` -->
<!-- - **10** `yeats` -->

<!-- Outputs for closeness, betweeness, eigenvector centralities and degree measurements of the abstracts network for Task 5. -->

```{r}
sort(-closeness(abs_net))
sort(-betweenness(abs_net))
sort(evcent(abs_net)$vector)
sort(degree(abs_net))
```

<!-- Outputs for closeness, betweeness, eigenvector centralities and degree measurements of the tokens matrix for Task 6. -->

```{r}
sort(-closeness(tok_net))
sort(-betweenness(tok_net))
sort(evcent(tok_net)$vector)
sort(degree(tok_net))
```

<!-- Degrees of vertices of the bipartite graph in Task 7. -->

```{r}
sort(degree(bipart))
```











