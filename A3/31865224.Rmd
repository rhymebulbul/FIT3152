---
urlcolor: blue
output: pdf_document
---
# FIT3152 Data analytics

## Assignment 3

**Name:** Rhyme Bulbul

**Student ID:** 31865224

**AI statement:** 



## Task 1
I gathered a collection of 19 text documents from different sources that covered a range of themes for this task. Blogs, news stories, movie reviews, and more are included in these publications. The Appendix contains the list of references for these documents.

Text files were created by copying and pasting the document contents.


## Task 2
Task 1 already had the document's contents pasted into text files. The text files were assembled into the `text` folder within the working directory in order to generate the corpus. Next, the code that follows is executed. It installs and imports the necessary libraries, sets the seed and uses the `Corpus()` function from the `tm` package to construct the corpus.

```{r message = FALSE}
rm(list = ls())
set.seed(31865224)
#install.packages("tm")
library(tm)
#install.packages("cluster")
library(cluster)
#install.packages("igraph")
library(igraph)

cname <- file.path(".", "text")
docs <- Corpus(DirSource((cname)))
list.files("text")
```

Documents are named based on the topic, suffixed by a number. Such as, `linux01.txt` is the first text document on linux systems, followed by `linux02.txt` the second text document on the same topic, and so fourth.


## Task 3

We start by text transforming the corpus, meanwhile replacing dashes and line breaks with spaces for consistency, removing numbers, punctuation, converting all characters to lowercase, and removing any extra white spaces. In addition, we also remove English stopwords before finally stemming all words for consistency. This is required as we don't want these characters creating an unwanted bias in our data, as it would be harder to work with, as well as inaccurate.

```{r}
to_space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, to_space, "-")
docs <- tm_map(docs, to_space, "\n")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument, language = "english")
```

With our unwanted terms removed, and desired keywords preserverd, the corpus is now ready to create a document term matrix from.

```{r}
dtm <- DocumentTermMatrix(docs)
```

Let's analyse the attributes of the document term matrix by inspecting a sample of 20 from it's head and tail of most and least frequent terms in alphabetical order.

```{r}
inspect(dtm)
```

```{r}
freq <- colSums(as.matrix(dtm))
freq[head(order(freq), 20)]
```

```{r}
freq[tail(order(freq), 20)]
```

With 6062 terms, or as we call it, tokens; the DTM is highly sparse at 88%. Interestingly, the least occurring tokens seem to only appear once, while the most frequent token is `android`, occurring almost 500 times, although it is only expected to be used in 3 documents, pointing to it's sparsity.

This leads us to the removal of sparse tokens from the document term matrix. This time we set the sparsity to 0.7 as it gives us the best mix of efficiency, reliability, and observability.

<!-- The next step is to remove sparse terms from the DTM. The maximal allowed sparsity, `sparse`, is set to 0.65, which gives a DTM with 54% sparsity and 32 tokens. A DTM closer to the guideline with 51% sparsity and 23 tokens can be obtained by setting `sparse` to 0.6, but through my analysis I observed that the 32-token DTM gives a better cluster dendrogram and network graph. **I also discovered that setting the sparsity even higher further improves the clustering, but I avoided this due to efficiency issues and poor readability of graphs.** -->

```{r}
dtms <- removeSparseTerms(dtm, 0.15)
inspect(dtms)
freqs <- colSums(as.matrix(dtms))
freqs[head(order(freqs), 20)]
freqs[tail(order(freqs), 20)]
```

This time round, the least frequent tokens are `exact`, `speed`, `pass` and `care` while the most frequent is `use` with 259 uses. The Document term matrix in it's full length can be found attached in the appendix.


## Task 4

<!-- A hierarchical clustering of the corpus is done based on two metrics, Euclidean distance and cosine distance, to see which performs better. The DTM is first converted to a standard matrix format. -->

```{r}
dtms_matrix <- as.matrix(dtms)
```

<!-- In document clustering, each document is represented as a vector with many dimensions that correspond to terms it contains. Euclidean distance measures the straight-line distance between these vectors, grouping them based on closeness. A smaller distance represents higher similarity. Cosine distance measures the angle between vectors, with smaller angles corresponding to closer distance and higher similarity. Clustering with cosine distance can be further improved by weighting the DTM with the term frequency-inverse document frequency (TF-IDF) statistic beforehand, which assigns higher weights to terms that appear frequently within a document (implying importance), but rarely across all documents (implying significance). -->

<!-- The code for clustering using Euclidean distance is adapted from Lecture 10. `dtms_matrix` is scaled and converted to a Euclidean distance matrix, and a dendrogram is plotted. -->

```{r fig.height = 5, fig.width = 7}
dist_euclid <- dist(scale(dtms_matrix))
fit_euclid <- hclust(dist_euclid, method = "ward.D")
plot(fit_euclid, hang = -1)
```

<!-- To cluster with TF-IDF weighting and cosine distance, the IDF for each term is first computed and a TF-IDF weighted matrix is obtained by applying the cross product of the TFs and IDFs. The formula for cosine distance is then applied to the matrix to get a cosine distance matrix. The dendrogram is then plotted. The code for this was adapted from [this Stack Overflow thread](https://stackoverflow.com/questions/52391558/hierarchical-clustering-using-cosine-distance-in-r). The code was studied and understood before application. -->

```{r fig.height = 5, fig.width = 7}
idf <- log(ncol(dtms_matrix) / (1 + rowSums(dtms_matrix != 0)))
idf <- diag(idf)
dtms_matrix_tfidf <- crossprod(dtms_matrix, idf)
colnames(dtms_matrix_tfidf) <- rownames(dtms_matrix)

dist_cos <- 1 - crossprod(dtms_matrix_tfidf) / (sqrt(colSums(dtms_matrix_tfidf ** 2) %*%
            t(colSums(dtms_matrix_tfidf ** 2))))
dist_cos[is.na(dist_cos)] <- 0
dist_cos <- as.dist(dist_cos)

fit_cos <- hclust(dist_cos, method = "ward.D")
plot(fit_cos, hang = -1)
```

<!-- At the highest level in Euclidean distance clustering, `linux02` is in a single cluster while all other documents are grouped into another cluster. This indicates that the clustering algorithm identifies `linux02` as an outlier document with distinct characteristics. However, it does share a common topic with `linux01`, so this is an inaccurate and imbalanced result. For the rest of the documents, some documents with the same topics are clustered together at low levels, such as `churchill01` and `churchill02`. However, there are some prominent inaccuracies, such as `bitcoin01` and `bitcoin02` being clustered far from each other (only grouped together at the highest level) and `asioaf02` (an extract from a fantasy novel) being clustered together with `stats01` (an extract from a statistics textbook) at the lowest level. -->

<!-- With cosine distance, the clustering produces more balanced clusters. `linux02` is grouped with several other documents at the highest level, but it is still separated from `linux01`. However, this clustering is overall better at grouping documents of the same topic together. For example, `bitcoin01` and `bitcoin02` are closer together, while the pairs of `covid01`-`covid02` and `yeats01`-`yeats03` are each in their own clusters at the lowest level. The overall height values of the clustering are also much smaller (with Euclidean distance, the largest height was 28.6 compared to 2.28 for cosine distance), indicating higher confidence in the grouping of similar documents. Conclusively, this is a better clustering for the documents compared to that with Euclidean distance. -->

<!-- Since the actual topic of each document is known, it is possible to get a quantitative measure of each clustering by labelling each document with its topic, plotting a confusion matrix of the clustering, and computing the accuracy. Due to length, the confusion matrices and the cluster-topic assignments are shown in the Appendix (for Euclidean distance, 15 clusters are created to reduce topic-cluster ambiguity). -->

```{r results = "hide"}
# function to remove suffix in document filename
short_name <- function(doc) {
    return(substr(doc, 1, nchar(doc) - 6))
}

doc_names <- list.files("docs")
doc_names_short <- unlist

# table(Topic = doc_names_short, Cluster = cutree(fit_euclid, k = 10))
# table(Topic = doc_names_short, Cluster = cutree(fit_cos, k = 10))
```

<!-- The accuracies of each matrix is calculated by hand. For clustering with Euclidean distance, this is given by -->

```{r}
13 / 23
```

<!-- Whereas for cosine distance, this is given by -->

```{r}
15 / 23
```

<!-- In agreement with the observations, clustering with cosine distance has a significantly higher accuracy than clustering with Euclidean distance. -->

## Task 5












TODO: Have approx 20 tokens after removing sparse terms
Remove name suffixes
generate euclid and cos distances









































## Appendix


## References

Wikimedia Foundation. (2024, May 14). Ubuntu. Wikipedia. https://en.wikipedia.org/wiki/Ubuntu 

Wikimedia Foundation. (2024c, May 18). Linux kernel. Wikipedia. https://en.wikipedia.org/wiki/Linux_kernel 

Wikimedia Foundation. (2024a, April 5). Lineageos. Wikipedia. https://en.wikipedia.org/wiki/LineageOS 

Wikimedia Foundation. (2024b, May 7). Android (Operating System). Wikipedia. https://en.wikipedia.org/wiki/Android_(operating_system) 

Encyclopædia Britannica, inc. (2024, May 14). Australia. Encyclopædia Britannica. https://www.britannica.com/place/Australia 

Goode, L. (2024, May 14). It’s the end of google search as we know it. Wired. https://www.wired.com/story/google-io-end-of-google-search/ 

IMDb.com. (2003, June 6). 2 fast 2 furious. IMDb. https://www.imdb.com/title/tt0322259/?ref_=nv_sr_srsg_0_tt_7_nm_1_q_2%2520fast 

IMDb.com. (2009, April 3). Fast & Furious. IMDb. https://www.imdb.com/title/tt1013752/?ref_=nv_sr_srsg_1_tt_7_nm_0_q_fast%2520and%2520fur 

IMDb.com. (2003b, July 9). Pirates of the Caribbean: The curse of the black pearl. IMDb. https://www.imdb.com/title/tt0325980/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_pirates 

IMDb.com. (2017, May 26). Pirates of the caribbean: Dead men tell no tales. IMDb. https://www.imdb.com/title/tt1790809/?ref_=nv_sr_srsg_3_tt_8_nm_0_q_pirates 

IMDb.com. (2014, October 24). John Wick. IMDb. https://www.imdb.com/title/tt2911666/?ref_=nv_sr_srsg_1_tt_7_nm_1_q_jon%2520wick 

IMDb.com. (1996, May 22). Mission: Impossible. IMDb. https://www.imdb.com/title/tt0117060/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_mission 

IMDb.com. (2016, February 12). Deadpool. IMDb. https://www.imdb.com/title/tt1431045/?ref_=nv_sr_srsg_3_tt_6_nm_2_q_deadpool 

IMDb.com. (1984, October 26). The terminator. IMDb. https://www.imdb.com/title/tt0088247/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_terminator 

IMDb.com. (1977, May 25). Star wars: Episode IV - A new hope. IMDb. https://www.imdb.com/title/tt0076759/?ref_=nv_sr_srsg_4_tt_7_nm_0_q_star%2520wars 

IMDb.com. (2004, May 19). Shrek 2. IMDb. https://www.imdb.com/title/tt0298148/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_shrek%25202 

IMDb.com. (2006, October 20). The prestige. IMDb. https://www.imdb.com/title/tt0482571/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_prestige 

IMDb.com. (2013, December 25). The wolf of wall street. IMDb. https://www.imdb.com/title/tt0993846/?ref_=nm_flmg_t_38_prd 

IMDb.com. (2002, December 25). Catch me if you can. IMDb. https://www.imdb.com/title/tt0264464/?ref_=nv_sr_srsg_0_tt_8_nm_0_q_catch%2520me 




